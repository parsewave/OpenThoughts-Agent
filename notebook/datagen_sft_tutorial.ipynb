{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a9b829",
   "metadata": {},
   "source": [
    "# Data Generation and SFT Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bceee44",
   "metadata": {},
   "source": [
    "In this tutorial we showcase an example of how we generate tasks --> generate teacher traces --> perform SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed9514",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7829211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from data.commons import upload_tasks_to_hf, upload_traces_to_hf\n",
    "from scripts.harbor.run_and_export_traces import run_dataset_to_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bd8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned to: /tmp/user/24984/inferredbugs_etq4t4ac/InferredBugs\n"
     ]
    }
   ],
   "source": [
    "def clone_inferredbugs():\n",
    "    \"\"\"Clone InferredBugs repository to temp directory\"\"\"\n",
    "    temp_dir = Path(tempfile.mkdtemp(prefix=\"inferredbugs_\"))\n",
    "    repo_path = temp_dir / \"InferredBugs\"\n",
    "    \n",
    "    cmd = [\"git\", \"clone\", \"https://github.com/microsoft/InferredBugs.git\", str(repo_path)]\n",
    "    subprocess.run(cmd, check=True, capture_output=True)\n",
    "    \n",
    "    return repo_path\n",
    "\n",
    "repo_path = clone_inferredbugs()\n",
    "print(f\"Cloned to: {repo_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca8598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11186 bugs\n"
     ]
    }
   ],
   "source": [
    "def find_bugs(repo_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find all bug directories in csharp and java folders\"\"\"\n",
    "    bugs = []\n",
    "    \n",
    "    for language in [\"csharp\", \"java\"]:\n",
    "        lang_path = repo_path / \"inferredbugs\" / language\n",
    "        if not lang_path.exists():\n",
    "            continue\n",
    "        \n",
    "        for root, _, files in os.walk(lang_path):\n",
    "            if \"bug.json\" not in files:\n",
    "                continue\n",
    "                \n",
    "            bug_json_path = Path(root) / \"bug.json\"\n",
    "            try:\n",
    "                with open(bug_json_path, 'r', encoding='utf-8') as f:\n",
    "                    bug_data = json.load(f)\n",
    "                \n",
    "                # Read before/after code if available\n",
    "                additional = {}\n",
    "                for fname in [\"file_before.txt\", \"file_after.txt\", \"method_before.txt\", \"method_after.txt\"]:\n",
    "                    fpath = Path(root) / fname\n",
    "                    if fpath.exists():\n",
    "                        with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                            additional[fname.replace('.txt', '')] = f.read()\n",
    "                \n",
    "                bugs.append({\n",
    "                    'path': str(root),\n",
    "                    'language': language,\n",
    "                    'bug_data': bug_data,\n",
    "                    'additional_data': additional,\n",
    "                    'project_name': Path(root).parent.name,\n",
    "                    'bug_id': Path(root).name\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return bugs\n",
    "\n",
    "bugs = find_bugs(repo_path)\n",
    "print(f\"Found {len(bugs)} bugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841763f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11186 instructions\n"
     ]
    }
   ],
   "source": [
    "def create_instruction(bug: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format bug data into a task instruction\"\"\"\n",
    "    bug_data = bug['bug_data']\n",
    "    additional = bug.get('additional_data', {})\n",
    "    \n",
    "    instruction = f\"\"\"# Bug Fix Task - {bug['language'].title()}\n",
    "\n",
    "**Project:** {bug['project_name']}\n",
    "**Bug ID:** {bug['bug_id']}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if 'description' in bug_data:\n",
    "        instruction += f\"**Description:** {bug_data['description']}\\n\\n\"\n",
    "    \n",
    "    if 'file_before' in additional and 'file_after' in additional:\n",
    "        instruction += f\"### Code (Buggy Version):\\n```{bug['language']}\\n{additional['file_before']}\\n```\\n\\n\"\n",
    "    \n",
    "    instruction += \"## Task\\nFix the bug in the code.\\n\"\n",
    "    \n",
    "    return instruction\n",
    "\n",
    "instructions = [create_instruction(bug) for bug in bugs]\n",
    "print(f\"Created {len(instructions)} instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631bf7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating tasks in: /tmp/user/24984/inferredbugs_tasks_7r0jqr3r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11186it [00:02, 3999.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11186 tasks successfully!\n",
      "Tasks saved to: /tmp/user/24984/inferredbugs_tasks_7r0jqr3r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_tasks(instructions: List[str], dataset_name: str = \"inferredbugs\") -> Path:\n",
    "    \"\"\"Generate Harbor-compatible task directories\"\"\"\n",
    "    from data.commons import generate_tasks_from_questions\n",
    "    task_dir = generate_tasks_from_questions(instructions, dataset_name)\n",
    "    return Path(task_dir)\n",
    "\n",
    "task_dir = generate_tasks(instructions, \"inferredbugs\")\n",
    "print(f\"Tasks saved to: {task_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS cache disabled for subsample_tasks_directory; falling back to no-op caching: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "Creating subsampled tasks in: /tmp/user/24984/subsampled_g4l9mbqy\n",
      "Successfully subsampled 10 tasks from 11186 total tasks!\n",
      "Task Directory: /tmp/user/24984/subsampled_g4l9mbqy\n"
     ]
    }
   ],
   "source": [
    "def subsample_tasks(task_dir: Path, max_tasks: int = 100) -> Path:\n",
    "    \"\"\"Randomly subsample tasks for faster iteration\"\"\"\n",
    "    from data.commons import subsample_tasks_directory\n",
    "    subsampled = subsample_tasks_directory(str(task_dir), max_tasks)\n",
    "    return Path(subsampled)\n",
    "\n",
    "NUM_SAMPLES = 100 # TODO: Fill in the number of tasks to subsample to\n",
    "subsampled_task_dir = subsample_tasks(task_dir, max_tasks=NUM_SAMPLES)\n",
    "print(f\"Task Directory: {subsampled_task_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1376377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Fill in your API keys below\n",
    "# os.environ[\"DAYTONA_API_KEY\"] = \"\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044dfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for Jupyter notebook event loop conflict\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Teacher Traces\n",
    "\n",
    "def run_traces(\n",
    "    task_dir: Path,\n",
    "    model_name: str = \"gpt-5-mini\",\n",
    "    agent_name: str = \"terminus-2\",\n",
    "    n_concurrent: int = 8\n",
    "):\n",
    "    \"\"\"Run agent trials and collect traces\"\"\"\n",
    "    dataset = run_dataset_to_traces(\n",
    "        str(task_dir),\n",
    "        model_name=model_name,\n",
    "        agent_name=agent_name,\n",
    "        n_concurrent=n_concurrent,\n",
    "        agent_kwargs={\"max_episodes\": 3},\n",
    "        disable_verification=True\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "traces_dataset = run_traces(subsampled_task_dir, n_concurrent=8)\n",
    "print(f\"Collected {len(traces_dataset)} traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4690767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload tasks\n",
    "tasks_repo = \"\" # TODO: Fill in the HF tasks repo name\n",
    "upload_tasks_to_hf(str(subsampled_task_dir), tasks_repo)\n",
    "print(f\"✓ Tasks uploaded to {tasks_repo}\")\n",
    "\n",
    "# Upload traces\n",
    "traces_repo = \"\" # TODO: Fill in the HF traces repo name\n",
    "upload_traces_to_hf(traces_dataset, traces_repo, \"SFT\")\n",
    "print(f\"✓ Traces uploaded to {traces_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d2a8d",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c73f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"\" # TODO: Fill in the dataset name specified in the data generation step\n",
    "MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Small model for demo\n",
    "OUTPUT_DIR = \"outputs/inferredbugs_demo\"\n",
    "\n",
    "# Training hyperparameters\n",
    "# DeepSpeed config is not specified for this demo\n",
    "config = {\n",
    "    \"model_name_or_path\": MODEL,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"attn\": \"fa2\",\n",
    "    \"enable_liger_kernel\": True,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \n",
    "    # Method\n",
    "    \"stage\": \"sft\",\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"full\",\n",
    "    \"include_mfu\": True,\n",
    "    \n",
    "    # Dataset\n",
    "    \"dataset\": DATASET,\n",
    "    \"dataset_dir\": \"ONLINE\",  # Use online HuggingFace dataset\n",
    "    \"template\": \"qwen\",\n",
    "    \"cutoff_len\": 2048, # A small cutoff is used for demo purposes, for more effective training a larger cutoff should be used\n",
    "    \"overwrite_cache\": True,\n",
    "    \"preprocessing_num_workers\": 16,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"messages\": \"conversations\",\n",
    "    \"role_tag\": \"role\",\n",
    "    \"content_tag\": \"content\",\n",
    "    \"user_tag\": \"user\",\n",
    "    \"assistant_tag\": \"assistant\",\n",
    "    \n",
    "    # Training params\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 4.0e-5,\n",
    "    \"num_train_epochs\": 3.0,\n",
    "    \"max_grad_norm\": 1.0e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"logging_steps\": 1,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"plot_loss\": True,\n",
    "}\n",
    "\n",
    "print(f\"Dataset: {DATASET}\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = snapshot_download(repo_id=MODEL, repo_type=\"model\")\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "dataset_path = snapshot_download(repo_id=DATASET, repo_type=\"dataset\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Update config with downloaded paths\n",
    "config[\"model_name_or_path\"] = model_path\n",
    "config[\"dataset\"] = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79591f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "config_path = f\"{OUTPUT_DIR}/train_config.yaml\"\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58bcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e77a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add llamafactory to path (go up one directory from notebook/)\n",
    "llamafactory_path = os.path.abspath(\"../sft/llamafactory/src\")\n",
    "if llamafactory_path not in sys.path:\n",
    "    sys.path.insert(0, llamafactory_path)\n",
    "\n",
    "print(f\"Added to path: {llamafactory_path}\")\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set sys.argv for llamafactory (use absolute path)\n",
    "sys.argv = [\"train.py\", os.path.abspath(config_path)]\n",
    "\n",
    "from llamafactory.train.tuner import run_exp\n",
    "run_exp()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

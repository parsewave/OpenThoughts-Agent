### Local single-GPU config (derived from 32k_base.yaml)
### Removed: global_batch_size (HPC-only), deepspeed (not needed for 1 GPU)
### Added: gradient_accumulation_steps, per_device_train_batch_size

### model
model_name_or_path: Qwen/Qwen3-8B
trust_remote_code: true
attn: fa2
enable_liger_kernel: true
optim: adamw_torch_fused

### method
stage: sft
do_train: true
finetuning_type: lora
include_mfu: true

### dataset
template: qwen3
cutoff_len: 32768
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_persistent_workers: true
dataloader_pin_memory: true
dataloader_num_workers: 4
dataset: open-thoughts/OpenThoughts-Agent-v1-SFT
dataset_dir: ONLINE

### output
logging_steps: 5
save_strategy: "steps"
save_steps: 1500
save_total_limit: 1
load_best_model_at_end: false
logging_strategy: "steps"
plot_loss: true
output_dir: /root/output
push_to_hub: true
hub_strategy: "end"
hub_model_id: loukwave/openthoughts-qwen3-8b-lora

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 16
learning_rate: 4.0e-5
num_train_epochs: 7.0
max_grad_norm: 1.0e-4
adam_beta2: 0.98
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
pure_bf16: false
gradient_checkpointing: true
ddp_timeout: 180000000

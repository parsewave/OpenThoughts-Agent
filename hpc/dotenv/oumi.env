export WANDB_ENTITY=dogml
export WANDB_PROJECT=OpenThoughts-Agent
export DCFT=/home/benjamin/OpenThoughts-Agent
export HF_HUB_CACHE=$SCRATCH/hub
export CHECKPOINTS_DIR=$SCRATCH/checkpoints
export MODELS_DIR=$HF_HUB_CACHE
export DATASETS_DIR=$HF_HUB_CACHE
export TOKENIZED_DATASETS_DIR=$SCRATCH/tokenized_datasets
export DCFT_CONDA=$HOME/miniconda3
export DCFT_ACTIVATE_ENV="source $HOME/miniconda3/bin/activate llama-factory"
export EVALCHEMY_ACTIVATE_ENV="source $HOME/miniconda3/bin/activate evalchemy"
export VLLM_ACTIVATE_ENV="source $HOME/miniconda3/bin/activate vllm"
export NUM_NODES_SLOW=4
export NUM_NODES_DEFAULT=16
export NUM_NODES_FAST=32
export NUM_GPUS_PER_NODE=1
export DEFAULT_TIME_LIMIT=168:00:00
export MAX_TIME_LIMIT=168:00:00
export VLLM_CACHE_ROOT=$SCRATCH/vllm
export PINGGY_PERSISTENT_URL=xcdppeurjb.a.pinggy.link
export PINGGY_DEBUGGER_URL=http://localhost:4300/
export PINGGY_SSH_COMMAND='while true; do ssh -p 443 -R0:localhost:8000 -L4300:localhost:4300 -o StrictHostKeyChecking=no -o ServerAliveInterval=30 i7KxKVvGYVs@pro.pinggy.io; sleep 10; done'
export DC_AGENT_SECRET_ENV="/home/benjamin/.env_vars"
export GCS_CREDENTIALS_PATH="/home/benjamin/.gcs_credentials.json"
export PYTORCH_CUDA_ALLOW_TF32=1          # turns on TF32 for GEMM (same as torch.set_float32_matmul_precision("high"))
export PYTORCH_CUDNN_ALLOW_TF32=1         # optional, lets cuDNN kernels use TF32 too
# Keep the repo importable even if Python strips the working directory from sys.path.
export PYTHONPATH="${DCFT_PRIVATE:-$DCFT}${PYTHONPATH:+:$PYTHONPATH}"

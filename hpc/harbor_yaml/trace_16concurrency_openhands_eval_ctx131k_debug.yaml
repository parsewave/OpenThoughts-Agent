# Harbor eval configuration (debug, fast-fail, 16 concurrency, OpenHands, 131k context).

job_name: debug-openhands-eval-job
jobs_dir: trace_jobs
n_attempts: 1
timeout_multiplier: 1.0
debug: true

orchestrator:
  type: local
  n_concurrent_trials: 16
  quiet: false
  plain_output: true
  adaptive_concurrency:
    enabled: false
    algorithm: gradient2
    min_limit:
    max_limit:
    metrics_endpoint:
    metrics_timeout_sec: 10.0
    poll_interval_sec: 120.0
    window_size: 5
    queue_p95_drop_threshold:
    algorithm_kwargs: {}
  retry:
    max_retries: 10
    include_exceptions:
    - DaytonaRateLimitError
    exclude_exceptions:
    - AgentTimeoutError
    - VerifierTimeoutError
    wait_multiplier: 2.0
    min_wait_sec: 1.0
    max_wait_sec: 90.0
  kwargs: {}

environment:
  type: daytona
  force_build: true
  delete: true
  override_cpus: 1
  override_memory_mb: 2048
  override_storage_mb: 2048
  kwargs: {}

verifier:
  override_timeout_sec:
  max_timeout_sec:
  disable: false

metrics: []

agents:
- name: openhands
  import_path:
  model_name: placeholder/override-at-runtime
  max_timeout_sec:
  override_setup_timeout_sec: 600
  kwargs:
    disable_tool_calls: true
    collect_rollout_details: false
    collect_engine_metrics: false
    metrics_endpoint: https://replace-with-vllm-host/metrics
    metrics_timeout_sec: 10
    model_info:
      max_input_tokens: 131072
      max_output_tokens: 16384
      input_cost_per_token: 0
      output_cost_per_token: 0

    trajectory_config:
      raw_content: false
      linear_history: true

datasets:
- path: /replace/with/tasks/path

tasks: []

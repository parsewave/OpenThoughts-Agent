# SkyRL Configuration: Qwen3 8B on 16x80GB GPUs (RLOO-N + Error Classification)
# Optimized for 32k context agentic RL with ASYNCHRONOUS training + RLOO-N
#
# RLOO-N (RLOO-Neutral): A variant of RLOO that excludes infrastructure failures
# from the baseline computation. This addresses a key limitation in standard RLOO:
# - Infrastructure failures (DaytonaError, NetworkError) are treated as "neutral"
#   and excluded from the group baseline - they don't reflect agent quality.
# - Agent failures (timeout, context overflow) are included with reward=0 - the
#   model should learn to avoid these situations.
#
# REWARD MODE: Binary (0/1) - original verifier reward without shaping
# TRAINING MODE: Fully async - policy/ref/inference on dedicated GPU sets
# ADVANTAGE ESTIMATOR: RLOO-N (leave-one-out baseline, excludes masked samples)
#
# Hardware: 16 GPUs with 80GB VRAM each (e.g., 2x8 H100/A100-80GB or 4x4)
# Model: Qwen3 8B Base (~16GB bf16 weights)
#
# GPU ALLOCATION (colocate_all=false):
#   - Policy model: 8 GPUs (2 nodes x 4 GPUs)
#   - Reference model: 8 GPUs (2 nodes x 4 GPUs) - CPU offloaded, shares with policy
#   - Inference engines: 8 GPUs (8 engines x TP=1)
#
# ASYNC TRAINING + RLOO-N BENEFITS:
#   - No waiting for slowest trajectory in batch
#   - Continuous policy updates as trajectories complete
#   - Infrastructure failures don't pollute the baseline
#   - Better sample efficiency when some trials fail
#   - Better GPU utilization during generation
#   - Reduced wall-clock time for same number of samples
#
# CONTEXT BUDGET (32k total model context):
#   - max_generate_length: 8k tokens (agent responses)
#   - vLLM enforces total context <= max_model_len (32k)
#
# Key Design Decisions:
# - TP=1 for inference engines: 8 independent engines for maximum parallelism
# - batched=false: Each async worker handles its own requests
# - num_parallel_generation_workers=128: High parallelism for async training
# - n_samples_per_prompt=8: RLOO-N advantage estimation with more samples
# - enable_error_classification=true: Classify errors as infra vs agent failures
#
# Usage:
#   python -m hpc.launch \
#       --job_type rl \
#       --rl_config qwen3_8b_16x80GB_thinking_bs64_group8_async_rloo_n.yaml \
#       --job_name qwen3_8b_agentic_async_rloo_n \
#       --train_data '["mlfoundations-dev/your-dataset"]' \
#       --model_path Qwen/Qwen3-8B \
#       --num_nodes 2

# SkyRL entrypoint module
entrypoint: examples.terminal_bench.entrypoints.main_tbench

# Hydra config groups (+ prefix in CLI)
config_groups:
  terminal_bench_config: terminal_bench

# Terminal bench / agentic environment settings
terminal_bench:
  # trials_dir: Directory for Harbor trial artifacts (derived from experiments_dir if null)
  trials_dir: null

  # Harbor configuration - schema-driven mapping to TrialConfig
  # See examples/terminal_bench/harbor_config.py for full list of supported fields.
  # Unknown fields will generate warnings (typo protection + version compatibility).
  harbor:
    # ==========================================================================
    # Agent settings (maps to Harbor AgentConfig)
    # ==========================================================================
    # Agent name - must be a valid Harbor AgentName (e.g., terminus-2, oracle)
    name: terminus-2
    # Max agent-environment turns before Harbor forces termination
    max_episodes: 64
    # Disable summarization, consistent with prior RL work
    enable_summarize: false
    # Store full message history for trajectory reconstruction
    store_all_messages: true
    # Disable episode-* folder creation to reduce disk I/O (SkyRL uses TrialResult directly)
    enable_episode_logging: false

    # ==========================================================================
    # Interleaved Thinking Settings (KEY ABLATION)
    # ==========================================================================
    # Include reasoning content in chat history and send to LLM in next round
    interleaved_thinking: true
    # Enable thinking in the LLM via chat template kwargs
    extra_body:
      chat_template_kwargs:
        enable_thinking: true

    # Agent timeout overrides
    override_timeout_sec: 240        # Agent execution timeout (seconds)
    # override_setup_timeout_sec: 120  # Agent setup timeout
    # max_timeout_sec: 1800          # Cap timeout (useful for runaway tasks)

    # ==========================================================================
    # Environment settings (maps to Harbor EnvironmentConfig)
    # ==========================================================================
    # Sandbox resources - adequate for most agentic tasks
    override_cpus: 1
    override_memory_mb: 2048
    override_storage_mb: 2048
    # override_gpus: 0              # GPU allocation per sandbox
    # environment_type: daytona     # Environment backend (daytona, docker, modal)

    # ==========================================================================
    # Trial-level settings (maps to Harbor TrialConfig)
    # ==========================================================================
    # timeout_multiplier: 1.0       # Multiply all timeouts by this factor

    # ==========================================================================
    # Verifier settings (maps to Harbor VerifierConfig)
    # ==========================================================================
    # verifier_disable: false       # Disable verification (for debugging)
    verifier_override_timeout_sec: 120  # Override verifier timeout

    # ==========================================================================
    # Retry settings (maps to Harbor RetryConfig for QueueOrchestrator)
    # ==========================================================================
    # Retry logic with exponential backoff for transient failures
    max_retries: 3                 # Max retry attempts per trial
    min_wait_sec: 60.0              # Initial backoff delay (seconds)
    max_wait_sec: 600.0             # Maximum backoff delay
    wait_multiplier: 2.0           # Exponential backoff multiplier

    # Exception filtering - don't retry permanent failures
    # Default Harbor exclusions: AgentTimeoutError, VerifierTimeoutError,
    #   RewardFileNotFoundError, RewardFileEmptyError, VerifierOutputParseError
    # We add ContextLengthExceededError since summarization is disabled
    exclude_exceptions:
      - AgentTimeoutError
      - VerifierTimeoutError
      - RewardFileNotFoundError
      - RewardFileEmptyError
      - VerifierOutputParseError
      - ContextLengthExceededError  # Permanent when enable_summarize=false

    # ==========================================================================
    # Orchestrator settings (QueueOrchestrator concurrency control)
    # ==========================================================================
    n_concurrent_trials: 128       # Set at 1/2 of recommended for stability

    # ==========================================================================
    # Logging settings
    # ==========================================================================
    log_level: WARNING             # Harbor logging level (DEBUG, INFO, WARNING, ERROR)

    # ==========================================================================
    # Reward shaping settings (DISABLED - binary rewards)
    # ==========================================================================
    enable_reward_shaping: false   # Use original binary (0/1) verifier rewards

    # ==========================================================================
    # RLOO-N ERROR HANDLING: Classify failures as infrastructure vs agent errors
    # ==========================================================================
    # Enable error classification for RLOO-N advantage estimator.
    # When enabled, infrastructure failures are excluded from the baseline (neutral),
    # while agent failures are included with reward=0.
    enable_error_classification: true

    # Exceptions to MASK (exclude from baseline - infrastructure failures)
    # These don't reflect agent quality and shouldn't affect other samples' baselines.
    mask_exceptions:
      - DaytonaError                  # Daytona sandbox infrastructure failure
      - EnvironmentStartTimeoutError  # Environment failed to start
      - NetworkError                  # Network connectivity issues
      - ConnectionError               # Connection failures
      - RewardFileNotFoundError       # Verifier output missing (infra issue)
      - RewardFileEmptyError          # Verifier output empty (infra issue)

    # Exceptions to ZERO (include in baseline with reward=0 - agent failures)
    # These reflect agent behavior that should be learned to avoid.
    zero_exceptions:
      - AgentTimeoutError             # Agent took too long (should learn efficiency)
      - ContextLengthExceededError    # Agent used too much context (should learn brevity)

    # Default treatment for unclassified exceptions ("mask" or "zero")
    # Use "zero" to be conservative - treat unknown errors as agent failures
    default_error_treatment: zero

  # Model info for Harbor's hosted_vllm validation (token limits and costs)
  model_info:
    # Context budget: 32k total = 24k prompt + 8k generation
    # Prompts grow with conversation history, so reserve headroom for generation
    max_input_tokens: 24576   # 24k - leaves 8k for generation
    max_output_tokens: 8192   # 8k - sufficient for most agent responses

# Trainer configuration
trainer:
  strategy: fsdp2
  algorithm:
    # ==========================================================================
    # RLOO-N: REINFORCE Leave-One-Out with Neutral sample handling
    # ==========================================================================
    # RLOO-N computes baselines using only "included" samples:
    # - Samples with exclude_from_baseline=True are masked (advantage=0, no gradient)
    # - Baselines for other samples are computed WITHOUT the excluded samples
    # This prevents infrastructure failures from polluting the baseline.
    advantage_estimator: rloo_n
    use_kl_loss: true
    # KL loss coefficient - controls divergence from reference model
    kl_loss_coef: 0.001
    # Clip range for policy updates
    eps_clip_low: 0.2
    eps_clip_high: 0.2
    # Token-level loss for better gradient signal on long sequences
    loss_reduction: token_mean

  # Training loop settings
  epochs: 10
  update_epochs_per_batch: 1

  # Batch sizes - async training requires train_batch_size == policy_mini_batch_size
  # (no mini-batching in fully async mode)
  train_batch_size: 64
  policy_mini_batch_size: 64
  eval_batch_size: 32

  # Micro batch sizes - set to 1 for long context to avoid OOM
  micro_forward_batch_size_per_gpu: 1
  micro_train_batch_size_per_gpu: 1

  max_prompt_length: 999999

  # Evaluation and checkpointing
  eval_interval: 999999
  eval_before_train: false
  ckpt_interval: 10
  resume_mode: latest

  # Logging
  project_name: OpenThoughts-Agent
  log_level: INFO  # SkyRL logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  # Commit to WandB after each step (for real-time monitoring in async training)
  tracker_commit_each_step: true

  # Paths derived from experiments_dir/job_name if null
  run_name: null
  ckpt_path: null
  export_path: null

  # Policy optimizer - conservative LR for base model fine-tuning
  policy:
    optimizer_config:
      lr: 1e-6
      weight_decay: 0.01
      adam_betas: [0.9, 0.999]
      # Gradient clipping important for long sequences
      max_grad_norm: 1.0

  # Reference model - CPU offload to save GPU memory
  # With colocate_all=false, ref shares GPUs with policy but offloads weights
  ref:
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true

  # ==========================================================================
  # ASYNC TRAINING: Model placement (colocate_all=false)
  # ==========================================================================
  # Non-colocated placement for async RL:
  # - Policy and ref get dedicated GPU sets (can overlap with CPU offload)
  # - Inference engines run on separate GPUs
  # - No memory swapping between rollout and training phases
  placement:
    colocate_all: false
    policy_num_nodes: 2
    ref_num_nodes: 2
    policy_num_gpus_per_node: 4
    ref_num_gpus_per_node: 4

  # ==========================================================================
  # ASYNC TRAINING: Fully async generation settings
  # ==========================================================================
  # Enable async RL training loop
  fully_async:
    # Number of parallel generation workers
    # Constraint: mini_batch_size <= num_parallel_generation_workers <= mini_batch_size * (max_staleness_steps + 1)
    # With mini_batch_size=16 and max_staleness_steps=4: 16 <= workers <= 80
    max_staleness_steps: 4
    num_parallel_generation_workers: 128

# Generator (vLLM inference) configuration
generator:
  backend: vllm
  model_dtype: bfloat16

  # ==========================================================================
  # ASYNC TRAINING: Inference engine configuration
  # ==========================================================================
  # TP=1 for maximum parallelism (8 independent engines)
  # Each engine has full 80GB for 32k context with single sequences
  inference_engine_tensor_parallel_size: 1
  # 8 engines = 8 GPUs dedicated to inference
  num_inference_engines: 8

  # RLOO-N samples per prompt - 8 for advantage estimation with more samples
  # More samples per prompt gives better leave-one-out baselines
  n_samples_per_prompt: 8
  eval_n_samples_per_prompt: 4

  # Memory utilization - aggressive but leave headroom for 32k KV cache
  gpu_memory_utilization: 0.90

  # Sequence limits for 32k context (24k prompt + 8k gen)
  # With TP=1, each engine handles fewer concurrent sequences
  max_num_seqs: 32
  # max_num_batched_tokens: tokens processed per batch
  max_num_batched_tokens: 32768

  # Memory optimization for long contexts
  enable_prefix_caching: true
  enable_chunked_prefill: true

  # ==========================================================================
  # ASYNC TRAINING: Engine settings
  # ==========================================================================
  run_engines_locally: true
  weight_sync_backend: nccl
  async_engine: true
  batched: false
  enable_http_endpoint: true

  # Multi-turn agentic conversation handling
  append_eos_token_after_stop_str_in_multi_turn: true
  max_turns: 64  # Match max_episodes

  # Sampling parameters for generation
  sampling_params:
    # Context budget: 32k total = 24k prompt + 8k generation
    # This caps generation to stay within model's max_model_len
    max_generate_length: 8192  # 8k - matches model_info.max_output_tokens
    # Temperature 1.0 for exploration during training
    temperature: 1.0
    top_p: 0.95
    top_k: -1  # Disabled

  # Engine initialization kwargs - passed directly to vLLM/SGLang engine
  # WARNING: Many kwargs are set by SkyRL internally (trust_remote_code, enforce_eager, etc.)
  # The launcher validates this at parse time and will error with a full list if you set forbidden keys.
  # SAFE TO SET: custom_chat_template_*, kv_cache_dtype, quantization, cpu_offload_gb, etc.
  engine_init_kwargs:
    # Explicit context limit - ensures vLLM enforces 32k total context
    max_model_len: 32768
    # Chat template for Qwen3 Base model (path relative to working directory or absolute)
    custom_chat_template_chat_completion_path: chat_templates/qwen3_thinking_acc.jinja2

# Data paths (set via CLI --train_data, --val_data)
data:
  train_data: []
  val_data: ["open-thoughts/OpenThoughts-TB-dev"]

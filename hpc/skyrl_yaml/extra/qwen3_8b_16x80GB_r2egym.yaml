# SkyRL Configuration: Qwen3 8B on 16x80GB GPUs (OpenHands-style)
# Replicates key settings from skyrl_oh.sh for R2E/SWE-bench style tasks
#
# KEY DIFFERENCES FROM qwen3_8b_16x80GB_shaped.yaml:
# 1. NO KL loss (use_kl_loss: false) - allows larger policy updates
# 2. Dual clip policy loss - more stable updates
# 3. MUCH shorter generation (2048 tokens) - forces concise responses
# 4. More samples per prompt (8 vs 4) - better advantage estimation
# 5. More turns allowed (50 vs 32) - more interaction steps
# 6. Higher learning rate (1.0e-6 vs 5.0e-7)
# 7. Non-batched generation (batched: false)
# 8. Binary rewards (no shaping) - matches R2E evaluation style
#
# Hardware: 16 GPUs with 80GB VRAM each (e.g., 2x8 H100/A100-80GB or 4x4)
# Model: Qwen3 8B Base (~16GB bf16 weights)
#
# CONTEXT BUDGET (32k total model context):
#   - max_generate_length: 2k tokens (agent responses)
#   - vLLM enforces total context <= max_model_len (32k)
#
# The shorter generation length is INTENTIONAL - it forces the model to
# be concise and prevents context budget exhaustion in long conversations.
#
# Usage:
#   python -m hpc.launch \
#       --job_type rl \
#       --rl_config qwen3_8b_16x80GB_openhands.yaml \
#       --job_name qwen3_8b_r2e \
#       --train_data '["mlfoundations-dev/your-dataset"]' \
#       --model_path Qwen/Qwen3-8B \
#       --num_nodes 2

# SkyRL entrypoint module
entrypoint: examples.terminal_bench.entrypoints.main_tbench

# Hydra config groups (+ prefix in CLI)
config_groups:
  terminal_bench_config: terminal_bench

# Terminal bench / agentic environment settings
terminal_bench:
  trials_dir: null

  harbor:
    # ==========================================================================
    # Agent settings - OpenHands uses 50 max iterations
    # ==========================================================================
    # Agent name - must be a valid Harbor AgentName (e.g., terminus-2, oracle)
    name: terminus-2
    max_episodes: 50
    enable_summarize: false
    store_all_messages: true
    # Disable episode-* folder creation to reduce disk I/O (SkyRL uses TrialResult directly)
    enable_episode_logging: false

    # Agent timeout - longer to accommodate more turns
    override_timeout_sec: 300  # 5 minutes per task

    # ==========================================================================
    # Environment settings
    # ==========================================================================
    override_cpus: 1
    override_memory_mb: 2048
    override_storage_mb: 2048

    # ==========================================================================
    # Verifier settings
    # ==========================================================================
    verifier_override_timeout_sec: 120

    # ==========================================================================
    # Retry settings
    # ==========================================================================
    max_retries: 3
    min_wait_sec: 60.0
    max_wait_sec: 600.0
    wait_multiplier: 2.0

    exclude_exceptions:
      - AgentTimeoutError
      - VerifierTimeoutError
      - RewardFileNotFoundError
      - RewardFileEmptyError
      - VerifierOutputParseError
      - ContextLengthExceededError

    # ==========================================================================
    # Orchestrator settings - OpenHands uses 80 parallel agents
    # We scale down proportionally: 80 agents / (32B/8B) ~= 20 agents
    # But limited by our 16 GPU setup
    # ==========================================================================
    n_concurrent_trials: 32  # Higher concurrency with shorter generations

    # ==========================================================================
    # Logging settings
    # ==========================================================================
    log_level: WARNING

    # ==========================================================================
    # Reward settings - BINARY rewards (OpenHands style)
    # OpenHands uses strict binary: match ALL expected test outcomes or 0
    # ==========================================================================
    enable_reward_shaping: false  # Binary rewards like OpenHands

  # Model info for Harbor's hosted_vllm validation
  model_info:
    # Context budget: 32k total = 30k prompt + 2k generation
    max_input_tokens: 30720   # 30k - more room for conversation history
    max_output_tokens: 2048   # 2k - MUCH shorter than our 8k default

# Trainer configuration - OpenHands style
trainer:
  strategy: fsdp2

  algorithm:
    advantage_estimator: grpo
    # KEY DIFFERENCE: No KL loss - allows larger policy updates
    use_kl_loss: false
    kl_loss_coef: 0.0
    # GRPO-specific settings
    grpo_norm_by_std: true
    # Dual clip policy loss (OpenHands uses this)
    policy_loss_type: dual_clip
    # Asymmetric clipping - OpenHands uses eps_clip_high=0.28
    eps_clip_low: 0.2
    eps_clip_high: 0.28
    # Token-level loss for better gradient signal on long sequences
    loss_reduction: token_mean

  # Training loop settings - OpenHands uses 100 epochs
  epochs: 100
  update_epochs_per_batch: 1

  # Batch sizes - OpenHands style
  # train_batch_size = n_samples_per_prompt * prompts_per_batch
  # With 8 samples per prompt and wanting 4 prompts: 8 * 4 = 32
  train_batch_size: 32  # Larger batches (OpenHands: 32)
  policy_mini_batch_size: 4  # Smaller mini-batches (OpenHands: 4)
  eval_batch_size: 256  # Match OpenHands eval batch size

  # Micro batch sizes - set to 1 for long context to avoid OOM
  micro_forward_batch_size_per_gpu: 1
  micro_train_batch_size_per_gpu: 1

  max_prompt_length: 999999

  # Evaluation and checkpointing - more frequent checkpoints
  eval_interval: 1000  # OpenHands: 1000
  eval_before_train: false
  ckpt_interval: 1  # Checkpoint every step (OpenHands style)
  hf_save_interval: 10  # Save HF format every 10 steps
  max_ckpts_to_keep: 10
  resume_mode: latest

  # Logging
  project_name: OpenThoughts-Agent-openhands
  log_level: INFO  # SkyRL logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

  # Paths derived from experiments_dir/job_name if null
  run_name: null
  ckpt_path: null
  export_path: null

  # Policy optimizer - higher LR (OpenHands: 1.0e-6)
  policy:
    optimizer_config:
      lr: 1.0e-6  # 2x our default (OpenHands uses this)
      weight_decay: 0.01
      adam_betas: [0.9, 0.999]
      max_grad_norm: 1.0
    # CPU offload for policy (OpenHands uses this)
    fsdp_config:
      cpu_offload: false

  # Reference model - CPU offload to save GPU memory
  ref:
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true

  # Model placement - derived from CLI args
  placement:
    colocate_all: true
    policy_num_nodes: null
    ref_num_nodes: null
    policy_num_gpus_per_node: null
    ref_num_gpus_per_node: null

# Generator (vLLM inference) configuration - OpenHands style
generator:
  backend: vllm
  model_dtype: bfloat16

  # Tensor parallelism = 4 (OpenHands: TP=4 for 32B model)
  # For 8B model we could use TP=2, but TP=4 gives more KV cache headroom
  # With 16 GPUs / TP=4 = 4 inference engines
  inference_engine_tensor_parallel_size: 4
  num_inference_engines: null  # Auto-computed: 16/4 = 4

  # MORE samples per prompt (OpenHands: 8)
  n_samples_per_prompt: 8
  eval_n_samples_per_prompt: 8

  # Memory utilization - less aggressive (OpenHands: 0.8)
  gpu_memory_utilization: 0.80

  # Sequence limits
  max_num_seqs: 64
  max_num_batched_tokens: 32768

  # Max input length for generator (OpenHands: 31232)
  max_input_length: 30720

  # Memory optimization
  enable_prefix_caching: true
  enable_chunked_prefill: true

  # Engine settings - OpenHands uses batched=false
  run_engines_locally: true
  weight_sync_backend: nccl
  async_engine: true
  batched: false  # KEY DIFFERENCE: non-batched generation (OpenHands style)
  enable_http_endpoint: true

  # Multi-turn agentic conversation handling
  append_eos_token_after_stop_str_in_multi_turn: true
  max_turns: 50  # Match max_episodes (OpenHands: 50)

  # Sampling parameters for generation
  sampling_params:
    # CRITICAL: Much shorter generation length (OpenHands: 2000)
    # Forces concise responses and prevents context bloat
    max_generate_length: 2048  # 2k - MUCH shorter than our 8k default
    temperature: 1.0
    top_p: 0.95
    top_k: -1

  # Engine initialization kwargs
  engine_init_kwargs:
    # Explicit context limit - ensures vLLM enforces 32k total context
    max_model_len: 32768
    custom_chat_template_chat_completion_path: chat_templates/qwen3_thinking_acc.jinja2

# Data paths (set via CLI --train_data, --val_data)
data:
  train_data: []
  val_data: ["open-thoughts/OpenThoughts-TB-dev"]

#!/bin/bash
#SBATCH --account={account}
#SBATCH --qos=premium
#SBATCH --constraint="gpu&hbm80g"
#SBATCH --exclusive
#SBATCH --nodes {num_nodes}
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task={cpus_per_node}
#SBATCH --gpus-per-node={gpus_per_node}
#SBATCH --time={time_limit}
#SBATCH --output={experiments_dir}/logs/%x_%j.out
#SBATCH --job-name={job_name}
#SBATCH --mail-type=END,TIME_LIMIT,FAIL
#SBATCH --mail-user=bf996@nyu.edu

set -euo pipefail

JOB_SUBMIT_ORDER=${JOB_SUBMIT_ORDER:-}
if [ -n "$JOB_SUBMIT_ORDER" ]; then
  if [[ "$JOB_SUBMIT_ORDER" =~ ^[0-9]+$ ]]; then
    SLEEP_DURATION=$((30 * JOB_SUBMIT_ORDER))
    echo "Submit order delay: sleeping ${SLEEP_DURATION}s (JOB_SUBMIT_ORDER=${JOB_SUBMIT_ORDER})"
    sleep "$SLEEP_DURATION"
  else
    echo "Warning: JOB_SUBMIT_ORDER='${JOB_SUBMIT_ORDER}' is not an integer; skipping launch delay"
  fi
fi

if [ -n "${DCFT:-}" ] && [ -f "$DCFT/hpc/dotenv/perlmutter.env" ]; then
  # shellcheck disable=SC1090
  source "$DCFT/hpc/dotenv/perlmutter.env"
elif [ -n "${DC_AGENT:-}" ] && [ -f "$DC_AGENT/hpc/dotenv/perlmutter.env" ]; then
  # shellcheck disable=SC1090
  source "$DC_AGENT/hpc/dotenv/perlmutter.env"
fi

module load cudatoolkit/12.9

if [ -n "${DCFT_ACTIVATE_ENV:-}" ]; then
  set +u
  eval "$DCFT_ACTIVATE_ENV"
  set -u
fi

# Ensure Triton build variables are set (used by vLLM/vLLM workers)
if [ -z "${TRITON_LIBCUDA_PATH:-}" ]; then
  for candidate in \
    "/usr/lib/x86_64-linux-gnu/libcuda.so.1" \
    "/usr/lib64/libcuda.so.1" \
    "/usr/local/cuda/lib64/stubs/libcuda.so" \
    "/usr/local/cuda/compat/libcuda.so.1"; do
    if [ -f "$candidate" ]; then
      export TRITON_LIBCUDA_PATH="$candidate"
      echo "Found libcuda at: $candidate"
      break
    fi
  done
fi
if [ -z "${TRITON_LIBCUDA_PATH:-}" ]; then
  ldconfig_path=$(ldconfig -p 2>/dev/null | awk '/libcuda\.so/ {print $NF; exit}')
  if [ -n "$ldconfig_path" ] && [ -f "$ldconfig_path" ]; then
    export TRITON_LIBCUDA_PATH="$ldconfig_path"
    echo "Detected libcuda via ldconfig: $TRITON_LIBCUDA_PATH"
  else
    fallback_lib="/usr/lib64/libcuda.so.1"
    if [ -f "$fallback_lib" ]; then
      export TRITON_LIBCUDA_PATH="$fallback_lib"
      echo "WARNING: Falling back to $fallback_lib for TRITON_LIBCUDA_PATH"
    else
      echo "WARNING: Unable to locate libcuda.so; Triton may fail to compile kernels." >&2
      export TRITON_LIBCUDA_PATH=""
    fi
  fi
fi

if command -v gcc >/dev/null 2>&1; then
  export TRITON_CC="$(command -v gcc)"
elif [ -x "/usr/bin/gcc" ]; then
  export TRITON_CC="/usr/bin/gcc"
else
  echo "WARNING: gcc not found in PATH; TRITON_CC will be empty." >&2
  export TRITON_CC=""
fi
echo "TRITON_CC: ${TRITON_CC:-<unset>}"
echo "TRITON_LIBCUDA_PATH: ${TRITON_LIBCUDA_PATH:-<unset>}"

SECRET_FILE="${DC_AGENT_SECRET_ENV:-${KEYS:-}}"
if [[ -n "${SECRET_FILE}" ]]; then
    if [[ -f "${SECRET_FILE}" ]]; then
        echo "Sourcing secrets from ${SECRET_FILE}"
        set -a
        # shellcheck disable=SC1090
        source "${SECRET_FILE}"
        set +a
    else
        echo "Warning: secrets file ${SECRET_FILE} not found; trace uploads may fail." >&2
    fi
else
    echo "Warning: DC_AGENT_SECRET_ENV is not set; trace uploads may fail." >&2
fi

export PYTHONFAULTHANDLER=1
export CUDA_LAUNCH_BLOCKING=0
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=INFO
export NCCL_IB_TIMEOUT=23
export HF_HOME=${HF_HOME:-$HF_HUB_CACHE}
export OUTLINES_CACHE_DIR="${OUTLINES_CACHE_DIR:-/tmp/.outlines}"
export TRITON_CACHE_DIR="${TRITON_CACHE_DIR:-$SCRATCH/triton_cache}"

if [ -z "${DCFT:-}" ]; then
  if [ -n "${DC_AGENT:-}" ]; then
    export DCFT="$DC_AGENT"
  else
    export DCFT="$PWD"
  fi
fi

mkdir -p "{experiments_dir}"
mkdir -p "{experiments_dir}/logs"

cd "$DCFT"
export PYTHONPATH="$PWD:${PYTHONPATH:-}"

# Ray GPU visibility:
# vLLM's Ray executor expects CUDA ordinals to match Ray's GPU ids. On some
# clusters, Ray's default CUDA_VISIBLE_DEVICES rewriting can remap ordinals and
# trigger "CUDA error: invalid device ordinal" inside vLLM workers.
export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES="${RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES:-1}"
export RAY_NOSET_CUDA_VISIBLE_DEVICES="${RAY_NOSET_CUDA_VISIBLE_DEVICES:-1}"

SRUN_EXPORT_ENV="ALL,PATH=$PATH,LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-},PYTHONPATH=$PYTHONPATH,HF_HOME=$HF_HOME,RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=$RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES,RAY_NOSET_CUDA_VISIBLE_DEVICES=$RAY_NOSET_CUDA_VISIBLE_DEVICES,TRITON_CC=${TRITON_CC:-},TRITON_LIBCUDA_PATH=${TRITON_LIBCUDA_PATH:-}"
RAY_ENV_VARS="PATH=$PATH LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-} PYTHONPATH=$PYTHONPATH HF_HOME=$HF_HOME RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=$RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES RAY_NOSET_CUDA_VISIBLE_DEVICES=$RAY_NOSET_CUDA_VISIBLE_DEVICES"

# Keep Ray sockets short by using /tmp on compute nodes but archive sessions to SCRATCH.
RAY_TMPDIR_BASE="${PERLMUTTER_RAY_TMPDIR_BASE:-/tmp/ray}"
RAY_TMPDIR="${RAY_TMPDIR_BASE}/ray_${SLURM_JOB_ID:-$$}"
mkdir -p "$RAY_TMPDIR"
RAY_SESSION_ARCHIVE_BASE="${PERLMUTTER_RAY_SESSION_ARCHIVE_BASE:-${SCRATCH:-$HOME}/ray_sessions_archive}"
RAY_SESSION_ARCHIVE="${RAY_SESSION_ARCHIVE_BASE}/${SLURM_JOB_ID:-$$}"
mkdir -p "$RAY_SESSION_ARCHIVE"
export RAY_SESSION_ARCHIVE
export TMPDIR="$RAY_TMPDIR"
export TEMP="$RAY_TMPDIR"
export TMP="$RAY_TMPDIR"

RAY_MEMORY_GB="${PERLMUTTER_RAY_MEMORY_GB:-200}"
RAY_OBJECT_STORE_GB="${PERLMUTTER_RAY_OBJECT_STORE_GB:-60}"
RAY_MEMORY_BYTES=$(python3 - <<PY
mem_gb = float("${RAY_MEMORY_GB}")
print(int(mem_gb * 1024**3))
PY
)
RAY_OBJECT_STORE_BYTES=$(python3 - <<PY
store_gb = float("${RAY_OBJECT_STORE_GB}")
print(int(store_gb * 1024**3))
PY
)

SRUN_EXPORT_ENV="$SRUN_EXPORT_ENV,RAY_TMPDIR=$RAY_TMPDIR,RAY_SESSION_TMPDIR=$RAY_TMPDIR,TMPDIR=$RAY_TMPDIR,TEMP=$RAY_TMPDIR,TMP=$RAY_TMPDIR,RAY_MEMORY_BYTES=$RAY_MEMORY_BYTES,RAY_OBJECT_STORE_BYTES=$RAY_OBJECT_STORE_BYTES"
RAY_ENV_VARS="$RAY_ENV_VARS RAY_TMPDIR=$RAY_TMPDIR RAY_SESSION_TMPDIR=$RAY_TMPDIR TMPDIR=$RAY_TMPDIR TEMP=$RAY_TMPDIR TMP=$RAY_TMPDIR RAY_MEMORY_BYTES=$RAY_MEMORY_BYTES RAY_OBJECT_STORE_BYTES=$RAY_OBJECT_STORE_BYTES"
echo "Ray session directory: $RAY_TMPDIR (archive: $RAY_SESSION_ARCHIVE)"
echo "Ray memory limits: total=${RAY_MEMORY_GB}GiB (${RAY_MEMORY_BYTES} bytes), object store=${RAY_OBJECT_STORE_GB}GiB (${RAY_OBJECT_STORE_BYTES} bytes)"

TRACE_BACKEND="${TRACE_BACKEND:-vllm}"

# Trace generation parameters
TRACE_SCRIPT="${TRACE_SCRIPT}"
TRACE_STAGE="${TRACE_STAGE:-traces}"
TRACE_TASKS_PATH="${TRACE_TASKS_PATH}"
TRACE_TARGET_REPO="${TRACE_TARGET_REPO}"
TRACE_MODEL="${TRACE_MODEL:-}"
TRACE_ENGINE="${TRACE_ENGINE:-}"
TRACE_OUTPUT_DIR="${TRACE_OUTPUT_DIR:-}"
TRACE_USE_GPU="${TRACE_USE_GPU:-0}"
TRACE_EPISODES="${TRACE_EPISODES:-}"
TRACE_EXPORT_FILTER="${TRACE_EXPORT_FILTER:-}"
TRACE_DATASET_TYPE="${TRACE_DATASET_TYPE:-}"
TRACE_JOBS_DIR="${TRACE_JOBS_DIR:-}"
TRACE_ENDPOINT_JSON="${TRACE_ENDPOINT_JSON:-$DCFT/vllm_endpoint.json}"
TRACE_REQUIRE_ENDPOINT="${TRACE_REQUIRE_ENDPOINT:-0}"
TRACE_WAIT_FOR_ENDPOINT="${TRACE_WAIT_FOR_ENDPOINT:-0}"
TRACE_HARBOR_CONFIG="${TRACE_HARBOR_CONFIG:-}"
TRACE_DISABLE_VERIFICATION="${TRACE_DISABLE_VERIFICATION:-0}"
TRACE_EVAL_ONLY="${TRACE_EVAL_ONLY:-0}"
TRACE_AGENT_TIMEOUT_SEC="${TRACE_AGENT_TIMEOUT_SEC:-}"
TRACE_VERIFIER_TIMEOUT_SEC="${TRACE_VERIFIER_TIMEOUT_SEC:-}"
TRACE_ENGINE_CONFIG_PATH="${TRACE_ENGINE_CONFIG_PATH:-}"
VLLM_JOB_ID="${VLLM_JOB_ID:-}"
TRACE_TASK_TYPE="${TRACE_TASK_TYPE:-}"
TRACE_EXPORT_SUBAGENTS="${TRACE_EXPORT_SUBAGENTS:-1}"
TRACE_AGENT_NAME="${TRACE_AGENT_NAME:-}"
TRACE_AGENT_KWARGS="${TRACE_AGENT_KWARGS:-}"
TRACE_ENV="${TRACE_ENV:-}"
TRACE_N_CONCURRENT="${TRACE_N_CONCURRENT:-}"
TRACE_CHUNK_SIZE="${TRACE_CHUNK_SIZE:-}"

source "$DCFT/hpc/sbatch_data/trace_helpers.sh"
load_trace_chunk_env

cleanup_vllm() {
  if [ -n "$VLLM_JOB_ID" ]; then
    echo "Tearing down VLLM job $VLLM_JOB_ID"
    scancel "$VLLM_JOB_ID" >/dev/null 2>&1 || echo "Warning: failed to cancel VLLM job $VLLM_JOB_ID"
  fi
}

trap cleanup_vllm EXIT

echo "=== Trace Generation Configuration ==="
echo "Backend: $TRACE_BACKEND"
echo "Trace Script: $TRACE_SCRIPT"
echo "Stage: $TRACE_STAGE"
echo "Tasks Input: $TRACE_TASKS_PATH"
echo "Target Repo: $TRACE_TARGET_REPO"
echo "Model: ${TRACE_MODEL:-<none>}"
echo "Output Dir: ${TRACE_OUTPUT_DIR:-<none>}"
echo "Engine: ${TRACE_ENGINE:-<none>}"
echo "Use GPU: $TRACE_USE_GPU"
echo "Task Type: ${TRACE_TASK_TYPE:-<none>}"
echo "Episodes: ${TRACE_EPISODES:-<none>}"
echo "Export Filter: ${TRACE_EXPORT_FILTER:-<none>}"
echo "Dataset Type: ${TRACE_DATASET_TYPE:-<none>}"
echo "Jobs Dir: ${TRACE_JOBS_DIR:-<none>}"
echo "Harbor Config: ${TRACE_HARBOR_CONFIG:-<none>}"
echo "Disable Verification: $TRACE_DISABLE_VERIFICATION"
echo "Eval Only: $TRACE_EVAL_ONLY"
if [ "${TRACE_CHUNK_MODE:-0}" = "1" ]; then
  echo "Chunk Index: ${TRACE_JOB_INDEX:-${SLURM_ARRAY_TASK_ID:-?}} / ${TRACE_CHUNK_COUNT:-?}"
fi
echo "======================================"

if [ -f "$TRACE_ENDPOINT_JSON" ]; then
    echo "Removing stale endpoint JSON: $TRACE_ENDPOINT_JSON"
    rm -f "$TRACE_ENDPOINT_JSON"
fi

ensure_tasks_ready_or_exit "$TRACE_TASKS_PATH" "${TRACE_TASKS_READY_TIMEOUT:-1800}"

if [ -n "$TRACE_OUTPUT_DIR" ]; then
    mkdir -p "$TRACE_OUTPUT_DIR"
fi

CMD=(python3 "$TRACE_SCRIPT" --stage "$TRACE_STAGE" --target-repo "$TRACE_TARGET_REPO" --tasks-input "$TRACE_TASKS_PATH")

if [ -n "$TRACE_OUTPUT_DIR" ]; then
    CMD+=(--output-dir "$TRACE_OUTPUT_DIR")
fi
if [ -n "$TRACE_HARBOR_CONFIG" ]; then
    CMD+=(--trace-harbor-config "$TRACE_HARBOR_CONFIG")
fi
if [ -n "$TRACE_JOBS_DIR" ]; then
    CMD+=(--trace-jobs-dir "$TRACE_JOBS_DIR")
fi
if [ -n "$TRACE_MODEL" ]; then
    CMD+=(--trace-model "$TRACE_MODEL")
fi
if [ -n "$TRACE_EPISODES" ]; then
    CMD+=(--trace-episodes "$TRACE_EPISODES")
fi
if [ -n "$TRACE_EXPORT_FILTER" ]; then
    CMD+=(--trace-export-filter "$TRACE_EXPORT_FILTER")
fi
if [ -n "$TRACE_DATASET_TYPE" ]; then
    CMD+=(--trace-dataset-type "$TRACE_DATASET_TYPE")
fi
if [ "$TRACE_EXPORT_SUBAGENTS" = "1" ]; then
    CMD+=(--trace-export-subagents)
else
    CMD+=(--trace-skip-subagents)
fi
if [ -n "$TRACE_ENGINE" ]; then
    CMD+=(--engine "$TRACE_ENGINE")
fi
if [ -n "$TRACE_AGENT_TIMEOUT_SEC" ]; then
  CMD+=(--trace-agent-timeout-sec "$TRACE_AGENT_TIMEOUT_SEC")
fi
if [ -n "$TRACE_VERIFIER_TIMEOUT_SEC" ]; then
  CMD+=(--trace-verifier-timeout-sec "$TRACE_VERIFIER_TIMEOUT_SEC")
fi
if [ "$TRACE_DISABLE_VERIFICATION" = "1" ]; then
    CMD+=(--disable-verification)
fi
if [ "$TRACE_EVAL_ONLY" = "1" ]; then
    CMD+=(--trace-eval-only)
fi
if [ -n "$TRACE_ENGINE_CONFIG_PATH" ]; then
    CMD+=(--engine-config "$TRACE_ENGINE_CONFIG_PATH")
fi
if [ -n "$TRACE_AGENT_NAME" ]; then
    CMD+=(--trace-agent-name "$TRACE_AGENT_NAME")
fi
if [ -n "$TRACE_AGENT_KWARGS" ]; then
    CMD+=(--trace-agent-kwargs "$TRACE_AGENT_KWARGS")
fi
if [ -n "$TRACE_ENV" ]; then
    CMD+=(--trace-env "$TRACE_ENV")
fi
if [ -n "$TRACE_N_CONCURRENT" ]; then
    CMD+=(--trace-n-concurrent "$TRACE_N_CONCURRENT")
fi
if [ -n "$TRACE_CHUNK_SIZE" ]; then
    CMD+=(--chunk_size "$TRACE_CHUNK_SIZE")
fi
if [ -n "$TRACE_TASK_TYPE" ]; then
    CMD+=(--trace-task-type "$TRACE_TASK_TYPE")
fi
if [ "$TRACE_BACKEND" = "vllm" ]; then
    CMD+=(--trace-backend vllm)
fi
if [ -n "$TRACE_ENDPOINT_JSON" ]; then
    CMD+=(--endpoint-json "$TRACE_ENDPOINT_JSON")
fi

run_trace() {
    echo "Command: ${CMD[*]}"
    echo "======================================"
    "${CMD[@]}"
    return $?
}

if [ "$TRACE_BACKEND" != "ray" ]; then
    if [ "$TRACE_REQUIRE_ENDPOINT" = "1" ] && [ "$TRACE_WAIT_FOR_ENDPOINT" = "1" ]; then
        if [ -z "$TRACE_ENDPOINT_JSON" ]; then
            echo "ERROR: Trace endpoint required but path is empty"
            exit 1
        fi

        echo "Waiting for trace endpoint JSON..."
        TRACE_MAX_WAIT=600
        TRACE_WAIT=0
        while [ ! -f "$TRACE_ENDPOINT_JSON" ] && [ $TRACE_WAIT -lt $TRACE_MAX_WAIT ]; do
            sleep 5
            TRACE_WAIT=$((TRACE_WAIT + 5))
            if [ $((TRACE_WAIT % 30)) -eq 0 ]; then
                echo "Still waiting for $TRACE_ENDPOINT_JSON... (${TRACE_WAIT}s elapsed)"
            fi
        done

        if [ ! -f "$TRACE_ENDPOINT_JSON" ]; then
            echo "ERROR: Trace endpoint JSON not found after ${TRACE_MAX_WAIT}s: $TRACE_ENDPOINT_JSON"
            exit 1
        fi

        echo "âœ“ Trace endpoint JSON present"
        cat "$TRACE_ENDPOINT_JSON"

        python3 scripts/vllm/wait_for_endpoint.py \
            --endpoint-json "$TRACE_ENDPOINT_JSON" \
            --max-attempts "${TRACE_HEALTH_MAX_ATTEMPTS:-20}" \
            --retry-delay "${TRACE_HEALTH_RETRY_DELAY:-60}" \
            --health-path "v1/models"
    else
        echo "Trace endpoint wait skipped (engine=$TRACE_ENGINE, wait_flag=$TRACE_WAIT_FOR_ENDPOINT)"
    fi

    run_trace
    EXIT_CODE=$?
else
    echo "=== Starting Ray-backed vLLM cluster for trace generation ==="
    nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
    nodes_array=($nodes)
    head_node=${nodes_array[0]}

    if [ -n "${PERLMUTTER_SRUN_MEM_MB:-}" ]; then
        MEM_PER_NODE_MB="$PERLMUTTER_SRUN_MEM_MB"
    else
        MEM_PER_NODE_MB=$(python3 - <<'PY'
import os,re
raw = os.environ.get("SLURM_MEM_PER_NODE", "262144")
m = re.match(r"([0-9]+)", raw)
print(m.group(1) if m else "262144")
PY
        )
    fi
    HEADROOM_MB="${PERLMUTTER_SRUN_HEADROOM_MB:-8192}"
    if [ "$MEM_PER_NODE_MB" -le "$HEADROOM_MB" ]; then
        SRUN_MEM_PER_STEP="$MEM_PER_NODE_MB"
    else
        SRUN_MEM_PER_STEP=$((MEM_PER_NODE_MB - HEADROOM_MB))
    fi

    SRUN_MEM_ARGS=()
    if [ -n "${PERLMUTTER_SRUN_MEM_MB:-}" ] || [ "${PERLMUTTER_SRUN_ENFORCE_MEM:-0}" = "1" ]; then
        SRUN_MEM_ARGS=(--mem="$SRUN_MEM_PER_STEP")
        echo "srun memory per step (enforced): ${SRUN_MEM_PER_STEP} MB (headroom ${HEADROOM_MB} MB, per-node ${MEM_PER_NODE_MB} MB)"
    else
        echo "srun memory per step target (not enforced by srun): ${SRUN_MEM_PER_STEP} MB (headroom ${HEADROOM_MB} MB, per-node ${MEM_PER_NODE_MB} MB)"
        echo "  Set PERLMUTTER_SRUN_ENFORCE_MEM=1 to make srun reserve this memory."
    fi

    head_node_ip=$(srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 "${SRUN_MEM_ARGS[@]}" --overlap -w "$head_node" hostname --ip-address)
    head_node_ip=${head_node_ip%% *}

    RAY_PORT="${TRACE_RAY_PORT:-6379}"
    API_PORT="${TRACE_API_PORT:-8000}"
    TP_SIZE="${TRACE_TENSOR_PARALLEL_SIZE:-1}"
    PP_SIZE="${TRACE_PIPELINE_PARALLEL_SIZE:-${VLLM_PIPELINE_PARALLEL_SIZE:-1}}"
    DP_SIZE="${TRACE_DATA_PARALLEL_SIZE:-1}"
    GPUS_PER_NODE="${TRACE_GPUS_PER_NODE:-0}"
    export TRACE_GPUS_PER_NODE="$GPUS_PER_NODE"
    NUM_NODES=$SLURM_JOB_NUM_NODES
    TOTAL_GPUS=$(python3 - <<'PY'
import os
print(int(os.environ.get("SLURM_JOB_NUM_NODES", "1")) * int(os.environ.get("TRACE_GPUS_PER_NODE", "0")))
PY
)

    ip_head="${head_node_ip}:${RAY_PORT}"
    export RAY_ADDRESS="$ip_head"

    ray_pids=()

    start_ray_node() {
        local node_name=$1
        local start_cmd=$2
        srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 "${SRUN_MEM_ARGS[@]}" --overlap -w "$node_name" bash -c "env $RAY_ENV_VARS $start_cmd" &
        ray_pids+=($!)
    }

    head_cmd="ray start --head --node-ip-address=${head_node_ip} --port=${RAY_PORT} --num-gpus=${GPUS_PER_NODE} --num-cpus={cpus_per_node} --temp-dir=${RAY_TMPDIR} --memory=${RAY_MEMORY_BYTES} --object-store-memory=${RAY_OBJECT_STORE_BYTES} --block"
    echo "Starting Ray head on ${head_node} (${head_node_ip})"
    start_ray_node "$head_node" "$head_cmd"

    for ((i = 1; i < NUM_NODES; i++)); do
        node_i=${nodes_array[$i]}
        worker_cmd="ray start --address ${ip_head} --num-gpus=${GPUS_PER_NODE} --num-cpus={cpus_per_node} --temp-dir=${RAY_TMPDIR} --memory=${RAY_MEMORY_BYTES} --object-store-memory=${RAY_OBJECT_STORE_BYTES} --block"
        echo "Starting Ray worker on ${node_i}"
        start_ray_node "$node_i" "$worker_cmd"
        sleep 3
    done

    cleanup_ray() {
        echo "Cleaning up Ray cluster and vLLM controller..."
        if [ -n "${VLLM_PID:-}" ]; then
            kill "$VLLM_PID" >/dev/null 2>&1 || true
            wait "$VLLM_PID" >/dev/null 2>&1 || true
        fi
        for node in "${nodes_array[@]}"; do
            srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 "${SRUN_MEM_ARGS[@]}" --overlap -w "$node" ray stop --force >/dev/null 2>&1 || true
        done
        for pid in "${ray_pids[@]}"; do
            wait "$pid" >/dev/null 2>&1 || true
        done
    }

    persist_ray_sessions() {
        echo "Archiving Ray sessions to $RAY_SESSION_ARCHIVE"
        mkdir -p "$RAY_SESSION_ARCHIVE"
        for node in "${nodes_array[@]}"; do
            local node_dest="$RAY_SESSION_ARCHIVE/$node"
            srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 "${SRUN_MEM_ARGS[@]}" --overlap -w "$node" bash -c '
                if [ -d "${RAY_TMPDIR:-}" ]; then
                    dest="'"$node_dest"'"
                    mkdir -p "$dest"
                    rsync -a "${RAY_TMPDIR}/" "$dest/" >/dev/null 2>&1 || true
                fi
            ' || true
        done
    }

    trap 'persist_ray_sessions; cleanup_ray; cleanup_vllm' EXIT

    echo "Waiting for Ray cluster resources..."
    python3 scripts/ray/wait_for_cluster.py \
        --address "$ip_head" \
        --expected-gpus "$TOTAL_GPUS" \
        --expected-nodes "$NUM_NODES" \
        --timeout 600 \
        --poll-interval 10

    echo "Launching vLLM Ray controller on ${head_node_ip}:${API_PORT}"
    controller_log="{experiments_dir}/logs/{job_name}_vllm.log"
    CONTROLLER_ENV_CMD="env PATH=$PATH LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-} HF_HOME=$HF_HOME PYTHONPATH=$PYTHONPATH"

    srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 "${SRUN_MEM_ARGS[@]}" --overlap -w "$head_node" \
        $CONTROLLER_ENV_CMD \
        python3 scripts/vllm/start_vllm_ray_controller.py \
            --ray-address "$ip_head" \
            --host "$head_node_ip" \
            --port "$API_PORT" \
            --endpoint-json "$TRACE_ENDPOINT_JSON" \
            --tensor-parallel-size "$TP_SIZE" \
            --pipeline-parallel-size "$PP_SIZE" \
            --data-parallel-size "$DP_SIZE" \
        >> "$controller_log" 2>&1 &
    VLLM_PID=$!

    echo "vLLM controller PID: $VLLM_PID (log: $controller_log)"

    if [ "$TRACE_REQUIRE_ENDPOINT" = "1" ] && [ "$TRACE_WAIT_FOR_ENDPOINT" = "1" ]; then
        ENDPOINT_WAIT_TIMEOUT="${TRACE_ENDPOINT_WAIT_TIMEOUT:-180}"
        ENDPOINT_WAIT_INTERVAL="${TRACE_ENDPOINT_WAIT_INTERVAL:-5}"
        wait_elapsed=0

        while true; do
            if ! kill -0 "$VLLM_PID" 2>/dev/null; then
                wait "$VLLM_PID" || true
                echo "ERROR: vLLM controller exited before writing endpoint JSON; see $controller_log"
                exit 1
            fi
            if [ -f "$TRACE_ENDPOINT_JSON" ]; then
                break
            fi
            if [ $wait_elapsed -ge $ENDPOINT_WAIT_TIMEOUT ]; then
                echo "ERROR: Timed out waiting for vLLM endpoint JSON at $TRACE_ENDPOINT_JSON"
                exit 1
            fi
            sleep "$ENDPOINT_WAIT_INTERVAL"
            wait_elapsed=$((wait_elapsed + ENDPOINT_WAIT_INTERVAL))
        done

        python3 scripts/vllm/wait_for_endpoint.py \
            --endpoint-json "$TRACE_ENDPOINT_JSON" \
            --max-attempts "${TRACE_HEALTH_MAX_ATTEMPTS:-60}" \
            --retry-delay "${TRACE_HEALTH_RETRY_DELAY:-15}" \
            --health-path "v1/models"
    else
        echo "Skipping endpoint readiness wait (engine=$TRACE_ENGINE, wait=$TRACE_WAIT_FOR_ENDPOINT)"
    fi

    run_trace
    EXIT_CODE=$?
fi

if [ $EXIT_CODE -eq 0 ]; then
    echo "Trace generation completed successfully."
else
    echo "Trace generation failed with exit code $EXIT_CODE."
fi

exit $EXIT_CODE

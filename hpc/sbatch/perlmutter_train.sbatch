#!/bin/bash
# Perlmutter training template for HPC launch.
#SBATCH --account={account}
#SBATCH --qos={qos}         # Other options: debug, shared
#SBATCH --constraint="gpu&hbm80g"
#SBATCH --exclusive
#SBATCH --nodes={num_nodes}
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task={cpus_per_node}
#SBATCH --gpus-per-node={gpus_per_node}
#SBATCH --time={time_limit}
#SBATCH --output={experiments_dir}/logs/%x_%j.out
#SBATCH --job-name={job_name}
#SBATCH --mail-type=END,TIME_LIMIT,FAIL
#SBATCH --mail-user=bf996@nyu.edu

set -euo pipefail

# Guard conda deactivate scripts from set -u complaints.
export CONDA_BACKUP_CXX="${CONDA_BACKUP_CXX:-}"
export CONDA_BACKUP_CC="${CONDA_BACKUP_CC:-}"
export CONDA_BACKUP_FC="${CONDA_BACKUP_FC:-}"

# Ensure dotenv variables are available on the compute node.
if [ -n "${DCFT:-}" ] && [ -f "$DCFT/hpc/dotenv/perlmutter.env" ]; then
  # shellcheck disable=SC1090
  source "$DCFT/hpc/dotenv/perlmutter.env"
fi

# Activate requested environment; fall back to manual setup if unset.
if [ -n "${DCFT_ACTIVATE_ENV:-}" ]; then
  set +u
  eval "$DCFT_ACTIVATE_ENV"
  set -u
else
  module load conda || true
  source "$HOME/.bashrc"
  set +u
  conda activate dcagent
  set -u
  if [ -f "$HOME/secrets.env" ]; then
    # shellcheck disable=SC1090
    source "$HOME/secrets.env"
  fi
fi

module load cudatoolkit/12.9

if [ -n "${LIBRARY_PATH:-}" ]; then
  export LIBRARY_PATH="$CUDA_HOME/lib64:$LIBRARY_PATH"
else
  export LIBRARY_PATH="$CUDA_HOME/lib64"
fi

if [ -n "${CUDA_HOME:-}" ]; then
  _cuda_sdk_root="$(dirname "$(dirname "$CUDA_HOME")")"
  _cuda_version="$(basename "$CUDA_HOME")"
  _cuda_math_lib_path="$_cuda_sdk_root/math_libs/$_cuda_version/lib64"
  if [ -d "$_cuda_math_lib_path" ]; then
    if [ -n "${LD_LIBRARY_PATH:-}" ]; then
      export LD_LIBRARY_PATH="$_cuda_math_lib_path:$LD_LIBRARY_PATH"
    else
      export LD_LIBRARY_PATH="$_cuda_math_lib_path"
    fi
  fi
  unset _cuda_sdk_root _cuda_version _cuda_math_lib_path
fi

if [ -n "${CONDA_PREFIX:-}" ]; then
  export CC="$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-gcc"
  export CXX="$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-g++"
  export FC="$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-gfortran"
  export CUDAHOSTCXX="$CXX"
  export PATH="$CONDA_PREFIX/bin:$PATH"
  if [ -n "${LD_LIBRARY_PATH:-}" ]; then
    export LD_LIBRARY_PATH="$CONDA_PREFIX/lib:$LD_LIBRARY_PATH"
  else
    export LD_LIBRARY_PATH="$CONDA_PREFIX/lib"
  fi
else
  echo "Warning: CONDA_PREFIX is not set; compiler environment variables were not configured." >&2
fi

echo "CUDA_HOME=${CUDA_HOME:-<unset>}"
echo "LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-<unset>}"

SECRET_FILE="${DC_AGENT_SECRET_ENV:-${KEYS:-}}"
if [[ -n "${SECRET_FILE}" ]]; then
    if [[ -f "${SECRET_FILE}" ]]; then
        echo "Sourcing secrets from ${SECRET_FILE}"
        set -a
        # shellcheck disable=SC1090
        source "${SECRET_FILE}"
        set +a
    else
        echo "Warning: secrets file ${SECRET_FILE} not found; database registration may fail." >&2
    fi
else
    echo "Warning: DC_AGENT_SECRET_ENV is not set; database registration may fail." >&2
fi

for _supabase_var in SUPABASE_URL SUPABASE_ANON_KEY SUPABASE_SERVICE_ROLE_KEY; do
    if [[ -n "${!_supabase_var:-}" ]]; then
        export "${_supabase_var}=${!_supabase_var}"
    else
        echo "Warning: ${_supabase_var} is not set; Supabase registration may fail." >&2
    fi
done

mkdir -p "$DCFT/{experiments_dir}"
mkdir -p "$DCFT/{experiments_dir}/logs"
TMP_DIR="$DCFT/{experiments_dir}/tmp"
mkdir -p "$TMP_DIR"

export PYTHONFAULTHANDLER=1
export CUDA_LAUNCH_BLOCKING=0
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=INFO
export NCCL_PROTO=simple
export NCCL_IB_TIMEOUT=22
export OMP_NUM_THREADS=1
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
if [ -n "${MASTER_PORT:-}" ]; then
  export MASTER_PORT="$MASTER_PORT"
else
  export MASTER_PORT=12802
fi
export NUM_NODES=$SLURM_JOB_NUM_NODES
export NUM_GPUS_PER_NODE={gpus_per_node}
export NUM_GPUS=$((NUM_GPUS_PER_NODE*SLURM_NNODES))
if [ -n "${HF_HOME:-}" ]; then
  export HF_HOME="$HF_HOME"
else
  export HF_HOME="$HF_HUB_CACHE"
fi
if [ -n "${WANDB_DIR:-}" ]; then
  export WANDB_DIR="$WANDB_DIR"
else
  export WANDB_DIR="$DCFT/{experiments_dir}/wandb"
fi

CONFIG="$DCFT/{train_config_path_out}"
OUTPUT_DIR="$DCFT/{experiments_dir}"
echo -e "CONFIG: $CONFIG\nOUTPUT_DIR: $OUTPUT_DIR"

# Deepspeed configuration is passed via HF TrainingArguments
DEEPSPEED_CONFIG_FILE={deepspeed}

CMD="torchrun \
    --nproc-per-node $NUM_GPUS_PER_NODE \
    --nnodes $NUM_NODES \
    --rdzv_id=$SLURM_JOB_ID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    sft/llamafactory/src/train.py $CONFIG"

SRUN_ARGS="
    --nodes=$NUM_NODES \
    --gpus-per-node=$NUM_GPUS_PER_NODE \
    --cpus-per-task=$SLURM_CPUS_PER_TASK \
    --wait=60 \
    --kill-on-bad-exit=1 \
    --label \
    --jobid $SLURM_JOBID"

export PYTHONPATH="$DCFT:${PYTHONPATH:-}"

cd "$DCFT"
srun $SRUN_ARGS bash -c "$CMD"

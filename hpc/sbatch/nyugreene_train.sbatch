#!/bin/bash
#SBATCH -A {account}
#SBATCH --constraint="h100"
#SBATCH --nodes={num_nodes}
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task={cpus_per_node}
#SBATCH --gres=gpu:{gpus_per_node}
#SBATCH --time={time_limit}
#SBATCH --mem={mem_per_node}
#SBATCH --output={experiments_dir}/logs/%x_%j.out
#SBATCH --job-name={job_name}
#SBATCH --mail-type=END,TIME_LIMIT,FAIL
#SBATCH --mail-user=${USER}@nyu.edu

set -euo pipefail

export DCFT=/scratch/$USER/dcagent_scratch

# Explicit dotenv path to avoid brittle expansions
export DCFT_DOTENV=${DCFT_DOTENV:-/scratch/$USER/OpenThoughts-Agent/hpc/dotenv/nyugreene.env}
source "$DCFT_DOTENV"

if [ -f "$DCFT_PRIVATE/database/access.env" ]; then
    source "$DCFT_PRIVATE/database/access.env"
else
    echo "[nyugreene_train] database/access.env not found; continuing without DB creds" >&2
fi

SECRET_FILE="${DC_AGENT_SECRET_ENV:-${KEYS:-}}"
if [[ -n "${SECRET_FILE}" ]]; then
    if [[ -f "${SECRET_FILE}" ]]; then
        echo "Sourcing secrets from ${SECRET_FILE}"
        set -a
        # shellcheck disable=SC1090
        source "${SECRET_FILE}"
        set +a
    else
        echo "Warning: secrets file ${SECRET_FILE} not found; database registration may fail." >&2
    fi
else
    echo "Warning: DC_AGENT_SECRET_ENV is not set; database registration may fail." >&2
fi

for _supabase_var in SUPABASE_URL SUPABASE_ANON_KEY SUPABASE_SERVICE_ROLE_KEY; do
    if [[ -n "${!_supabase_var:-}" ]]; then
        export "${_supabase_var}=${!_supabase_var}"
    else
        echo "Warning: ${_supabase_var} is not set; Supabase registration may fail." >&2
    fi
done

mkdir -p "{experiments_dir}"
mkdir -p "{experiments_dir}/logs"
TMP_DIR="{experiments_dir}/tmp"
mkdir -p "$TMP_DIR"

export PYTHONFAULTHANDLER=1
export CUDA_LAUNCH_BLOCKING=0
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=INFO
export NCCL_IB_TIMEOUT=22
# export NCCL_SOCKET_IFNAME="ib0"
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_PORT=${MASTER_PORT:-12802}
export NUM_NODES=$SLURM_JOB_NUM_NODES
export NUM_GPUS_PER_NODE={gpus_per_node}
export NUM_GPUS=$((NUM_GPUS_PER_NODE*SLURM_NNODES))
export HF_HOME=${HF_HOME:-$HF_HUB_CACHE}
WANDB_DIR_DEFAULT="{experiments_dir}/wandb"
export WANDB_DIR=${DCFT_WANDB_DIR:-$WANDB_DIR_DEFAULT}
export RUN_SINGULARITY=/scratch/$USER/OpenThoughts-Agent/hpc/scripts/run-singularity.bash

if [ ! -f "$RUN_SINGULARITY" ]; then
    echo "Missing singularity launcher at $RUN_SINGULARITY" >&2
    exit 1
fi

CONFIG={train_config_path_out}
OUTPUT_DIR={experiments_dir}
export CONFIG
export OUTPUT_DIR
echo -e "CONFIG: $CONFIG\nOUTPUT_DIR: $OUTPUT_DIR"

# Accelerate configuration
{accelerate_config_block}

# Build a small launcher script to avoid nested heredocs inside srun
TRAIN_SCRIPT="$TMP_DIR/${SLURM_JOB_ID}_train.sh.autogenerated"
cat > "$TRAIN_SCRIPT" <<'EOS'
#!/usr/bin/env bash
set -euo pipefail

cd "/scratch/$USER/OpenThoughts-Agent"
export PYTHONPATH="/scratch/$USER/OpenThoughts-Agent:${PYTHONPATH:-}"
export HF_HOME="/scratch/$USER/.cache/huggingface"
export CHECKPOINTS_DIR="/scratch/$USER/dcagent_scratch/checkpoints"
export WANDB_DIR="${OUTPUT_DIR}/wandb"
export TRITON_CACHE_DIR="${OUTPUT_DIR}/triton_cache"
export OUTLINES_CACHE_DIR="/tmp/.outlines"
export WANDB_INSECURE_DISABLE_SSL=true
export SSL_CERT_FILE=$(python -c "import certifi; print(certifi.where())")
python -u -m accelerate.commands.launch \
  --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT}" \
  --config_file "${ACCELERATE_CONFIG_FILE}" \
  --machine_rank "${SLURM_PROCID}" \
  --role "$(hostname -s)": --tee 3 \
  sft/llamafactory/src/train.py "${CONFIG}"
EOS
chmod +x "$TRAIN_SCRIPT"

# Run via Singularity wrapper
srun --cpu_bind=v --accel-bind=v bash -c "/bin/bash \"$RUN_SINGULARITY\" /bin/bash \"$TRAIN_SCRIPT\""

# Cleanup
rm -f "$ACCELERATE_CONFIG_FILE"
rm -f "$TRAIN_SCRIPT"

#!/bin/bash
set -euo pipefail

source ~/.bashrc
source ~/secrets.env
cd "$SCRATCH/OpenThoughts-Agent"
source hpc/dotenv/nyutorch.env
git pull --ff-only || true

# Ensure scripts module is importable
export PYTHONPATH="${PWD}:${PYTHONPATH:-}"

# =============================================================================
# Create conda activation script for Ray worker environment consistency
# This ensures spawned processes (Ray actors, multiprocessing workers) get
# the same environment as the main process.
# =============================================================================
echo "=== Setting up Conda Activation Script for Ray Workers ==="

# Determine conda env path
CONDA_ENV_NAME="dcagent"
if [[ -n "${CONDA_PREFIX:-}" ]]; then
    # If conda is already active, deactivate first
    conda deactivate 2>/dev/null || true
fi

# Find the conda env path
CONDA_ENV_PATH=""
for candidate in \
    "$SCRATCH/miniconda3/envs/$CONDA_ENV_NAME" \
    "$HOME/miniconda3/envs/$CONDA_ENV_NAME" \
    "$CONDA_PREFIX_1/envs/$CONDA_ENV_NAME" \
    "$(conda info --base 2>/dev/null)/envs/$CONDA_ENV_NAME"; do
    if [[ -d "$candidate" ]]; then
        CONDA_ENV_PATH="$candidate"
        break
    fi
done

if [[ -z "$CONDA_ENV_PATH" ]]; then
    echo "  WARNING: Could not locate conda env '$CONDA_ENV_NAME'; skipping activation script setup"
else
    ACTIVATE_D="$CONDA_ENV_PATH/etc/conda/activate.d"
    ACTIVATION_SCRIPT="$ACTIVATE_D/vllm_ray_env.sh"
    mkdir -p "$ACTIVATE_D"

    # Detect libcuda.so path for Triton
    TRITON_LIBCUDA_PATH=""
    for candidate in \
        "/usr/lib/x86_64-linux-gnu/libcuda.so.1" \
        "/usr/lib64/libcuda.so.1" \
        "/usr/lib64/libcuda.so" \
        "/usr/local/cuda/lib64/stubs/libcuda.so" \
        "/usr/local/cuda/compat/libcuda.so.1"; do
        if [[ -f "$candidate" ]]; then
            TRITON_LIBCUDA_PATH="$candidate"
            break
        fi
    done

    # Write the activation script
    cat > "$ACTIVATION_SCRIPT" << 'ACTIVATE_EOF'
#!/bin/bash
# =============================================================================
# vLLM Ray Worker Environment Setup
# Auto-generated by kimi-k2-tracegen-run.sh
# This script runs on conda activate to ensure Ray actors get consistent env
# =============================================================================

# Triton CUDA driver detection
if [[ -z "${TRITON_LIBCUDA_PATH:-}" ]]; then
    for _candidate in \
        "/usr/lib/x86_64-linux-gnu/libcuda.so.1" \
        "/usr/lib64/libcuda.so.1" \
        "/usr/lib64/libcuda.so" \
        "/usr/local/cuda/lib64/stubs/libcuda.so" \
        "/usr/local/cuda/compat/libcuda.so.1"; do
        if [[ -f "$_candidate" ]]; then
            export TRITON_LIBCUDA_PATH="$_candidate"
            break
        fi
    done
fi

# Ensure CUDA driver directory is in LD_LIBRARY_PATH
if [[ -n "${TRITON_LIBCUDA_PATH:-}" ]]; then
    _cuda_driver_dir=$(dirname "$TRITON_LIBCUDA_PATH")
    if [[ ":${LD_LIBRARY_PATH:-}:" != *":$_cuda_driver_dir:"* ]]; then
        export LD_LIBRARY_PATH="$_cuda_driver_dir:${LD_LIBRARY_PATH:-}"
    fi
fi

# GCC/Triton compiler settings (needed for JIT compilation)
if [[ -z "${TRITON_CC:-}" ]]; then
    if command -v gcc &>/dev/null; then
        export TRITON_CC=$(command -v gcc)
    fi
fi

# Ensure CUDA toolkit is in PATH and LD_LIBRARY_PATH
for _cuda_path in "/usr/local/cuda" "/usr/local/cuda-12" "/usr/local/cuda-12.8"; do
    if [[ -d "$_cuda_path/bin" ]] && [[ ":${PATH:-}:" != *":$_cuda_path/bin:"* ]]; then
        export PATH="$_cuda_path/bin:${PATH:-}"
    fi
    if [[ -d "$_cuda_path/lib64" ]] && [[ ":${LD_LIBRARY_PATH:-}:" != *":$_cuda_path/lib64:"* ]]; then
        export LD_LIBRARY_PATH="$_cuda_path/lib64:${LD_LIBRARY_PATH:-}"
    fi
done
ACTIVATE_EOF

    chmod +x "$ACTIVATION_SCRIPT"
    echo "  Created activation script: $ACTIVATION_SCRIPT"
    echo "  TRITON_LIBCUDA_PATH will be: ${TRITON_LIBCUDA_PATH:-<not found>}"
fi

# Now activate conda (which will source our new script)
conda activate "$CONDA_ENV_NAME"

# Verify the activation script worked
echo "=== Post-Activation Environment Check ==="
echo "  TRITON_CC=${TRITON_CC:-<not set>}"
echo "  TRITON_LIBCUDA_PATH=${TRITON_LIBCUDA_PATH:-<not set>}"
echo "  GCC: $(which gcc 2>/dev/null || echo '<not found>')"
echo "  libcuda.so exists: $([ -f "${TRITON_LIBCUDA_PATH:-/nonexistent}" ] && echo 'yes' || echo 'no')"
echo "  CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-<not set>}"
echo "  nvidia-smi GPU count: $(nvidia-smi -L 2>/dev/null | wc -l || echo 'nvidia-smi not available')"
echo "==========================================="

OVERWRITE_TASKS="${OVERWRITE_TASKS:-0}"
GPUS_PER_NODE="${GPUS_PER_NODE:-8}"
TRACE_EPISODES="${TRACE_EPISODES:-last}"
TRACE_EXPORT_FILTER="${TRACE_EXPORT_FILTER:-none}"
TRACE_DATASET_TYPE="${TRACE_DATASET_TYPE:-SFT}"

# Cap TIME_LIMIT at 47:59:00 (maximum for this job)
MAX_TIME_LIMIT="47:59:00"
TIME_LIMIT="${TIME_LIMIT:-$MAX_TIME_LIMIT}"

# Convert HH:MM:SS to seconds for comparison
time_to_seconds() {
  local t="$1"
  local h m s
  IFS=: read -r h m s <<< "$t"
  echo $(( 10#$h * 3600 + 10#$m * 60 + 10#$s ))
}

MAX_SECONDS=$(time_to_seconds "$MAX_TIME_LIMIT")
REQUESTED_SECONDS=$(time_to_seconds "$TIME_LIMIT")

if [[ "$REQUESTED_SECONDS" -gt "$MAX_SECONDS" ]]; then
  echo ">>> TIME_LIMIT $TIME_LIMIT exceeds maximum $MAX_TIME_LIMIT; capping to $MAX_TIME_LIMIT"
  TIME_LIMIT="$MAX_TIME_LIMIT"
fi

mkdir -p "$EXPERIMENTS_DIR" "$EXPERIMENTS_DIR/logs" "$EXPERIMENTS_DIR/outputs/traces" "$EXPERIMENTS_DIR/trace_jobs"

if [[ "$GPUS_PER_NODE" == "auto" || -z "$GPUS_PER_NODE" ]]; then
  if command -v nvidia-smi >/dev/null 2>&1; then
    GPU_ENUM=$(nvidia-smi -L 2>/dev/null || true)
    GPUS_PER_NODE=$(printf '%s\n' "$GPU_ENUM" | grep -c . || true)
  else
    GPUS_PER_NODE=0
  fi
fi

# Use short paths for Ray to avoid Unix socket path length limit (107 bytes max)
# The long experiment paths cause Ray to fail with socket path too long errors
RAY_SHORT_BASE="/tmp/ray_${SLURM_JOB_ID:-$$}"
RAY_TMPDIR="$RAY_SHORT_BASE"
RAY_SPILL_DIR="$EXPERIMENTS_DIR/ray_spill"  # Spill dir can be longer, it's not a socket
mkdir -p "$RAY_TMPDIR" "$RAY_SPILL_DIR"
export RAY_TMPDIR="$RAY_TMPDIR"
printf -v RAY_object_spilling_config '{"type":"filesystem","params":{"directory_path":"%s"}}' "$RAY_SPILL_DIR"
export RAY_object_spilling_config
echo ">>> Ray directories (using short paths to avoid socket limit):"
echo "    RAY_TMPDIR: $RAY_TMPDIR"
echo "    RAY_SPILL_DIR: $RAY_SPILL_DIR"

OBJECT_STORE_MEMORY_BYTES=$(python3 - <<'PY'
mem_total_bytes = None
try:
    with open("/proc/meminfo", encoding="utf-8") as fh:
        for line in fh:
            if line.startswith("MemTotal:"):
                parts = line.split()
                mem_total_bytes = int(parts[1]) * 1024  # value reported in KiB
                break
except Exception:
    mem_total_bytes = None

if mem_total_bytes:
    # Use only 5% of memory for object store to leave room for vLLM model loading
    # Kimi-K2-Thinking is a massive MoE model (~671B params) and needs significant CPU memory
    # vLLM also needs CPU memory for KV cache offloading and batching buffers
    target = int(mem_total_bytes * 0.05)
else:
    target = int(40 * 1024**3)  # fall back to 40 GiB

min_bytes = int(10 * 1024**3)
max_bytes = int(60 * 1024**3)  # cap at 60GB to leave room for model + vLLM buffers
target = max(min_bytes, min(target, max_bytes))
print(target)
PY
)

if [[ -z "$OBJECT_STORE_MEMORY_BYTES" ]]; then
  OBJECT_STORE_MEMORY_BYTES=$((40 * 1024 * 1024 * 1024))  # 40GB fallback
fi

if [[ "$GPUS_PER_NODE" == "0" ]]; then
  echo "WARNING: No GPUs detected on this node; Ray will start without GPU resources."
fi

echo ">>> Ray runtime dirs:"
echo "    TMP: $RAY_TMPDIR"
echo "    SPILL: $RAY_SPILL_DIR"
echo "    OBJECT STORE BYTES: $OBJECT_STORE_MEMORY_BYTES"

EXTRACT_CMD=(
  python3
  scripts/datagen/launch_trace_from_parquet.py
  --experiments_dir "$EXPERIMENTS_DIR"
  --datagen_config "$DATAGEN_CONFIG"
  --trace_script "$TRACE_SCRIPT"
  --trace_target_repo "$TRACE_TARGET_REPO"
  --trace_harbor_config "$TRACE_HARBOR_CONFIG"
  --trace_model "$TRACE_MODEL"
  --trace_engine "$TRACE_ENGINE"
  --trace_backend "$TRACE_BACKEND"
  --gpus_per_node "$GPUS_PER_NODE"
  --time_limit "$TIME_LIMIT"
  --tasks_repo "$TASKS_REPO"
  --extract_tasks_only
)

if [[ -n "${TASKS_REVISION:-}" ]]; then
  EXTRACT_CMD+=(--tasks_revision "$TASKS_REVISION")
fi
if [[ -n "${PARQUET_NAME:-}" ]]; then
  EXTRACT_CMD+=(--parquet_name "$PARQUET_NAME")
fi
if [[ "$OVERWRITE_TASKS" == "1" ]]; then
  EXTRACT_CMD+=(--overwrite)
fi

echo ">>> Extracting tasks via launch_trace_from_parquet.py"
echo "Command: ${EXTRACT_CMD[*]}"
"${EXTRACT_CMD[@]}"

TASKS_INPUT="$EXPERIMENTS_DIR/tasks_extracted"
TRACE_OUTPUT_DIR="$EXPERIMENTS_DIR/outputs/traces"
TRACE_JOBS_DIR="$EXPERIMENTS_DIR/trace_jobs"

echo ">>> Preparing datagen configuration snapshot and vLLM environment"
mapfile -t CONFIG_EXPORTS < <(python3 - <<'PY'
import os
import shlex
from hpc.datagen_launch_utils import (
    _prepare_datagen_configuration,
    _snapshot_datagen_config,
    _build_vllm_env_vars,
    resolve_datagen_config_path,
)

datagen_config = resolve_datagen_config_path(os.environ["DATAGEN_CONFIG"])
exp_args = {
    "datagen_config": str(datagen_config),
    "experiments_dir": os.environ["EXPERIMENTS_DIR"],
}
_prepare_datagen_configuration(exp_args)
snapshot_path = _snapshot_datagen_config(exp_args)
env_vars, exp_args = _build_vllm_env_vars(exp_args)

print(f'export DATAGEN_CONFIG_RESOLVED={shlex.quote(snapshot_path)}')
for key, value in env_vars.items():
    print(f'export {key}={shlex.quote(str(value))}')
ray_port = exp_args.get("datagen_ray_port", 6379)
api_port = exp_args.get("datagen_api_port", 8000)
print(f'export DATAGEN_RAY_PORT={shlex.quote(str(ray_port))}')
print(f'export DATAGEN_API_PORT={shlex.quote(str(api_port))}')
PY
)

for line in "${CONFIG_EXPORTS[@]}"; do
  eval "$line"
done

VLLM_ENDPOINT_JSON_PATH="${VLLM_ENDPOINT_JSON_PATH:-$EXPERIMENTS_DIR/vllm_endpoint.json}"
RAY_PORT="${DATAGEN_RAY_PORT:-6379}"
API_PORT="${DATAGEN_API_PORT:-8000}"
TP_SIZE="${VLLM_TENSOR_PARALLEL_SIZE:-1}"
PP_SIZE="${VLLM_PIPELINE_PARALLEL_SIZE:-1}"
DP_SIZE="${VLLM_DATA_PARALLEL_SIZE:-1}"

cleanup() {
  set +e
  echo ">>> Cleanup: stopping memory monitor, vLLM, and Ray..."
  if [[ -n "${MEMMON_PID:-}" ]]; then
    kill "$MEMMON_PID" >/dev/null 2>&1 || true
  fi
  if [[ -n "${VLLM_PID:-}" ]]; then
    kill "$VLLM_PID" >/dev/null 2>&1 || true
    wait "$VLLM_PID" >/dev/null 2>&1 || true
  fi
  ray stop --force >/dev/null 2>&1 || true
  echo ">>> Cleanup complete"
}
trap cleanup EXIT

# Start background memory monitor
MEMMON_LOG="$EXPERIMENTS_DIR/logs/memory_monitor.log"
(
  while true; do
    echo "=== $(date -Iseconds) ===" >> "$MEMMON_LOG"
    free -h >> "$MEMMON_LOG" 2>&1
    nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv >> "$MEMMON_LOG" 2>&1
    sleep 60
  done
) &
MEMMON_PID=$!
echo ">>> Started memory monitor (PID $MEMMON_PID, log: $MEMMON_LOG)"

HEAD_IP=$(python3 - <<'PY'
import socket
import subprocess

def first_ipv4(addresses: str) -> str | None:
    for candidate in addresses.split():
        if candidate and ":" not in candidate:
            return candidate
    return None

try:
    output = subprocess.check_output(["hostname", "-I"], text=True)
    ip = first_ipv4(output.strip())
    if ip:
        print(ip)
        exit(0)
except Exception:
    pass

try:
    ip = socket.gethostbyname(socket.gethostname())
    print(ip)
except Exception:
    print("127.0.0.1")
PY
)

if [[ "$HEAD_IP" == "127.0.0.1" || -z "$HEAD_IP" ]]; then
  echo "WARNING: unable to resolve non-loopback head IP (resolved '$HEAD_IP'); Ray may fail to accept worker connections."
fi

RAY_LOG="$EXPERIMENTS_DIR/logs/ray_head.log"
echo ">>> Stopping any existing Ray processes..."
ray stop --force 2>&1 || true
sleep 2

echo ">>> Starting Ray head on ${HEAD_IP}:${RAY_PORT}"
echo "    GPUs: $GPUS_PER_NODE, CPUs: ${SLURM_CPUS_PER_TASK:-32}"
echo "    Temp dir: $RAY_TMPDIR"
echo "    Object store memory: $OBJECT_STORE_MEMORY_BYTES bytes"
echo "    Log: $RAY_LOG"

# Start Ray head (without --block so we can check status)
# Temporarily disable set -e so we can capture the exit status
set +e
ray start --head \
  --node-ip-address="$HEAD_IP" \
  --port="$RAY_PORT" \
  --num-gpus="$GPUS_PER_NODE" \
  --num-cpus="${SLURM_CPUS_PER_TASK:-32}" \
  --dashboard-host 0.0.0.0 \
  --temp-dir="$RAY_TMPDIR" \
  --object-store-memory="$OBJECT_STORE_MEMORY_BYTES" \
  2>&1 | tee -a "$RAY_LOG"
RAY_START_STATUS=${PIPESTATUS[0]}
set -e

if [[ $RAY_START_STATUS -ne 0 ]]; then
  echo "ERROR: ray start failed with status $RAY_START_STATUS"
  echo ">>> Ray head log (last 100 lines):"
  tail -n 100 "$RAY_LOG" || true
  exit 1
fi

export RAY_ADDRESS="${HEAD_IP}:${RAY_PORT}"
echo ">>> Ray head started successfully. RAY_ADDRESS=$RAY_ADDRESS"

# Give Ray a moment to fully initialize
echo ">>> Waiting for Ray GCS to initialize..."
sleep 10

TOTAL_NODES=${SLURM_JOB_NUM_NODES:-1}
TOTAL_GPUS=$((GPUS_PER_NODE * TOTAL_NODES))

echo ">>> Waiting for Ray cluster (expecting $TOTAL_NODES nodes, $TOTAL_GPUS GPUs)..."
# Use stdbuf to disable buffering for python output
if ! stdbuf -oL -eL python3 -u scripts/ray/wait_for_cluster.py \
  --address "$RAY_ADDRESS" \
  --expected-gpus "$TOTAL_GPUS" \
  --expected-nodes "$TOTAL_NODES" \
  --timeout 600 \
  --poll-interval 10; then
  echo "ERROR: Ray cluster did not report expected resources"
  echo ">>> Ray head log (last 200 lines):"
  tail -n 200 "$RAY_LOG" || true
  echo ">>> Checking Ray status:"
  ray status 2>&1 || true
  exit 1
fi

echo ">>> Ray cluster is ready (nodes=$TOTAL_NODES, gpus=$TOTAL_GPUS)"
echo ">>> Memory status before vLLM launch:"
free -h 2>/dev/null || true
echo ">>> GPU memory status:"
nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv 2>/dev/null || true

CONTROLLER_LOG="$EXPERIMENTS_DIR/logs/vllm_controller.log"
rm -f "$VLLM_ENDPOINT_JSON_PATH"
echo ">>> Launching vLLM controller (log: $CONTROLLER_LOG)"
echo ">>> vLLM config: TP=$TP_SIZE, PP=$PP_SIZE, DP=$DP_SIZE"
python3 scripts/vllm/start_vllm_ray_controller.py \
  --ray-address "$RAY_ADDRESS" \
  --host "$HEAD_IP" \
  --port "$API_PORT" \
  --endpoint-json "$VLLM_ENDPOINT_JSON_PATH" \
  --tensor-parallel-size "$TP_SIZE" \
  --pipeline-parallel-size "$PP_SIZE" \
  --data-parallel-size "$DP_SIZE" \
  >>"$CONTROLLER_LOG" 2>&1 &
VLLM_PID=$!

ENDPOINT_WAIT=0
while [[ ! -f "$VLLM_ENDPOINT_JSON_PATH" && $ENDPOINT_WAIT -lt 900 ]]; do
  sleep 10
  ENDPOINT_WAIT=$((ENDPOINT_WAIT + 10))
  # Check if vLLM process is still alive
  if ! kill -0 "$VLLM_PID" 2>/dev/null; then
    echo "ERROR: vLLM controller process died (PID $VLLM_PID)" >&2
    echo ">>> Last 100 lines of controller log:" >&2
    tail -n 100 "$CONTROLLER_LOG" 2>/dev/null || true
    echo ">>> Memory status at crash:" >&2
    free -h 2>/dev/null || true
    exit 1
  fi
  echo "Waiting for vLLM endpoint JSON... ${ENDPOINT_WAIT}s (memory: $(free -h 2>/dev/null | awk '/^Mem:/{print $3"/"$2}' || echo 'N/A'))" >&2
done

if [[ ! -f "$VLLM_ENDPOINT_JSON_PATH" ]]; then
  echo "ERROR: vLLM endpoint JSON was not created within 900s" >&2
  echo ">>> Last 100 lines of controller log:" >&2
  tail -n 100 "$CONTROLLER_LOG" 2>/dev/null || true
  exit 1
fi

python3 scripts/vllm/wait_for_endpoint.py \
  --endpoint-json "$VLLM_ENDPOINT_JSON_PATH" \
  --max-attempts 30 \
  --retry-delay 10 \
  --health-path "v1/models"

TRACE_CMD=(
  python3 "$TRACE_SCRIPT"
  --stage traces
  --engine-config "$DATAGEN_CONFIG_RESOLVED"
  --target-repo "$TRACE_TARGET_REPO"
  --tasks-input "$TASKS_INPUT"
  --output-dir "$TRACE_OUTPUT_DIR"
  --trace-harbor-config "$TRACE_HARBOR_CONFIG"
  --trace-jobs-dir "$TRACE_JOBS_DIR"
  --trace-model "$TRACE_MODEL"
  --trace-engine "$TRACE_ENGINE"
  --trace-backend "$TRACE_BACKEND"
  --trace-episodes "$TRACE_EPISODES"
  --trace-export-filter "$TRACE_EXPORT_FILTER"
  --trace-dataset-type "$TRACE_DATASET_TYPE"
  --trace-use-gpu
)

if [[ -n "${TRACE_AGENT_NAME:-}" ]]; then
  TRACE_CMD+=(--trace-agent-name "$TRACE_AGENT_NAME")
fi
if [[ -n "${TRACE_AGENT_KWARGS:-}" ]]; then
  TRACE_CMD+=(--trace-agent-kwargs "$TRACE_AGENT_KWARGS")
fi
if [[ -n "${TRACE_ENV:-}" ]]; then
  TRACE_CMD+=(--trace-env "$TRACE_ENV")
fi
if [[ -n "${TRACE_AGENT_TIMEOUT_SEC:-}" ]]; then
  TRACE_CMD+=(--trace-agent-timeout-sec "$TRACE_AGENT_TIMEOUT_SEC")
fi
if [[ -n "${TRACE_MAX_TOKENS:-}" ]]; then
  TRACE_CMD+=(--trace-max-tokens "$TRACE_MAX_TOKENS")
fi
if [[ -n "${TRACE_N_CONCURRENT:-}" ]]; then
  TRACE_CMD+=(--trace-n-concurrent "$TRACE_N_CONCURRENT")
fi
if [[ -n "${TRACE_CHUNK_SIZE:-}" ]]; then
  TRACE_CMD+=(--chunk_size "$TRACE_CHUNK_SIZE")
fi

echo ">>> Running trace generator"
echo "Command: ${TRACE_CMD[*]}"
"${TRACE_CMD[@]}"

engine:
  type: vllm_local
  model: mistralai/Mistral-Large-3-675B-Instruct-2512-Eagle
  max_output_tokens: 16384
  healthcheck_interval: 300
  vllm_local: {}
backend:
  type: vllm
  wait_for_endpoint: true
  tensor_parallel_size: 8
  pipeline_parallel_size: 1
  healthcheck_max_attempts: 200
  healthcheck_retry_delay: 30
vllm_server:
  model_path: mistralai/Mistral-Large-3-675B-Instruct-2512-Eagle
  num_replicas: 1
  tensor_parallel_size: 8
  pipeline_parallel_size: 1
  time_limit: '166:00:00'
  max_model_len: 32768
  max_num_seqs: 16
  gpu_memory_utilization: 0.9
  swap_space: 32
  max_seq_len_to_capture: 32768
  trust_remote_code: true
  enable_auto_tool_choice: true
  tool_call_parser: mistral
  extra_args:
  - --tokenizer_mode
  - mistral
  - --config_format
  - mistral
  - --load_format
  - mistral
  - --kv-cache-dtype
  - fp8
  - --enable-prefix-caching
  - --max-num-batched-tokens
  - '8192'
  - --limit-mm-per-prompt
  - '{"image": 10}'
  - --speculative_config
  - '{"model": "mistralai/Mistral-Large-3-675B-Instruct-2512-Eagle", "num_speculative_tokens": 3, "method": "eagle", "max_model_len": 16384}'

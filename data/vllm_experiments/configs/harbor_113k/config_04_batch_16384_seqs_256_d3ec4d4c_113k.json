{
  "model_path": "QuantTrio/GLM-4.6-AWQ",
  "num_nodes": 8,
  "gpus_per_node": 1,
  "tensor_parallel_size": 8,
  "pipeline_parallel_size": 1,
  "num_replicas": 8,
  "enable_expert_parallel": true,
  "dtype": "bfloat16",
  "trust_remote_code": true,
  "block_size": 16,
  "swap_space_gb": 4,
  "enable_chunked_prefill": true,
  "max_num_partial_prefills": 1,
  "max_model_len": 113000,
  "name": "batch_16384_seqs_256_113k",
  "gpu_memory_utilization": 0.92,
  "max_num_batched_tokens": 16384,
  "max_num_seqs": 64,
  "kv_cache_dtype": "auto",
  "enable_prefix_caching": false,
  "all2all_backend": "pplx",
  "enable_eplb": false
}